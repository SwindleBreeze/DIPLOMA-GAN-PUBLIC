{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d97c5ff2-2086-4101-ac95-de69aeb086d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "random_seed = 1337\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "train_ratio = 0.8  # 80% for training, 20% for validation\n",
    "\n",
    "BATCH_SIZE=64\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "NUM_WORKERS=int(os.cpu_count() / 2) \n",
    "\n",
    "print(NUM_WORKERS)\n",
    "print(AVAIL_GPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aeaab97",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(failed_file)\n\u001b[0;32m     20\u001b[0m     failed_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mground_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "json_dir = '../learning-data/'\n",
    "\n",
    "all_segments = []\n",
    "all_grass_positions = []\n",
    "all_starting_points = []\n",
    "\n",
    "# Helper function to compute length and angle\n",
    "def compute_length_and_angle(x1, y1, x2, y2):\n",
    "    length = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "    angle = np.arctan2(y2 - y1, x2 - x1)\n",
    "    return length, angle\n",
    "\n",
    "failed_file = None\n",
    "# Iterate through JSON files\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        # delete failed file if it exists\n",
    "        if failed_file is not None:\n",
    "            os.remove(failed_file)\n",
    "            failed_file = None\n",
    "        with open(os.path.join(json_dir, filename), 'r') as f:\n",
    "            try:\n",
    "                ground_data = json.load(f)\n",
    "                segments = ground_data['segments']\n",
    "                grass_positions = ground_data['grassPositions']\n",
    "                # check if ground data contains starting point\n",
    "                if 'startingPoint' in ground_data:\n",
    "                    starting_point = ground_data['startingPoint']\n",
    "                else:\n",
    "                    starting_point = all_starting_points[-1]\n",
    "\n",
    "                # Append data to lists\n",
    "                all_segments.append(segments)\n",
    "                all_grass_positions.append(grass_positions)\n",
    "                all_starting_points.append(starting_point)\n",
    "            except:\n",
    "                print(f'Error reading {filename}')\n",
    "                # delete the file\n",
    "                failed_file = os.path.join(json_dir, filename)\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d878a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the data and save it to a file for later use\n",
    "with open('../learning-data/ground_data.pkl', 'wb') as f:\n",
    "    pickle.dump((all_segments, all_grass_positions, all_starting_points), f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "419989cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the serialized data\n",
    "with open('../learning-data/ground_data.pkl', 'rb') as f:\n",
    "    all_segments, all_grass_positions, all_starting_points = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86c61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_list = []\n",
    "for segment in all_segments:\n",
    "    #segment_tensor = torch.tensor([[point['x'], point['y'], point['length'], point['angleToNextVector']] for point in segment], dtype=torch.float32)\n",
    "    #segment_tensor = torch.tensor([[point['x'], point['y']] for point in segment], dtype=torch.float32)\n",
    "    segment_tensor = torch.tensor([[point['length'], point['angleToNextVector']] for point in segment], dtype=torch.float32)\n",
    "    segments_list.append(segment_tensor)\n",
    "segments_tensor = torch.stack(segments_list)\n",
    "\n",
    "# Convert grass positions to tensor\n",
    "grass_positions_tensor = torch.tensor(all_grass_positions, dtype=torch.int64)  # Assuming grass positions are integers\n",
    "\n",
    "# Convert starting points to tensor\n",
    "starting_points_tensor = torch.tensor(all_starting_points, dtype=torch.float32)\n",
    "\n",
    "# Serialize the tensors\n",
    "with open('../learning-data/ground_tensors.pkl', 'wb') as f:\n",
    "    pickle.dump((segments_tensor, grass_positions_tensor, starting_points_tensor), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6f8d12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 855, 2])\n"
     ]
    }
   ],
   "source": [
    "# Load the serialized tensors\n",
    "with open('../learning-data/ground_tensors.pkl', 'rb') as f:\n",
    "    segments_tensor, grass_positions_tensor, starting_points_tensor = pickle.load(f)\n",
    "    \n",
    "# keep only the first 5000 samples\n",
    "segments_tensor = segments_tensor[:5000]\n",
    "print(segments_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d048c698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 4000\n",
      "Validation dataset size: 1000\n"
     ]
    }
   ],
   "source": [
    "train_size = int((len(segments_tensor) * train_ratio) )# % for training\n",
    "val_size = int(len(segments_tensor) - train_size)  # Remaining % for validation\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_dataset, val_dataset = random_split(segments_tensor, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances for training and validation sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Optionally, you can print the sizes of the datasets\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1519101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model with dropout\n",
    "class FNNDiscriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, hidden_size4, output_size, dropout_prob=0.8):\n",
    "        super(FNNDiscriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc4 = nn.Linear(hidden_size3, hidden_size4)\n",
    "        self.fc5 = nn.Linear(hidden_size4, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.dropout(self.relu(self.fc3(x)))\n",
    "        x = self.dropout(self.relu(self.fc4(x)))\n",
    "        x = self.sigmoid(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "# Assuming input_size_discriminator = 1710\n",
    "\n",
    "\n",
    "# Define the generator model with dropout\n",
    "class LSTMGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, hidden_size4, output_size, dropout_prob=0.75):\n",
    "        super(LSTMGenerator, self).__init__()\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc2 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc3 = nn.Linear(hidden_size3, hidden_size4)\n",
    "        self.fc4 = nn.Linear(hidden_size4, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size1).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size1).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out = out[:, -1, :]  # Take the output from the last time step\n",
    "        \n",
    "        out = self.dropout(self.relu(self.fc1(out)))\n",
    "        out = self.dropout(self.relu(self.fc2(out)))\n",
    "        out = self.dropout(self.relu(self.fc3(out)))\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "    \n",
    "class FNNGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, hidden_size4, output_size, dropout_prob=0.8):\n",
    "        super(FNNGenerator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc4 = nn.Linear(hidden_size3, hidden_size4)\n",
    "        self.fc5 = nn.Linear(hidden_size4, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.dropout(self.relu(self.fc3(x)))\n",
    "        x = self.dropout(self.relu(self.fc4(x)))\n",
    "        x = self.tanh(self.fc5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fed1ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 855, 1710])\n",
      "torch.Size([64, 1710])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Shapes of real and fake data must match.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 172\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(normalized_real_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    171\u001b[0m fake_data \u001b[38;5;241m=\u001b[39m fake_data\u001b[38;5;241m.\u001b[39mview(fake_data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the fake data\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m real_data\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m fake_data\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes of real and fake data must match.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28mprint\u001b[39m(fake_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    174\u001b[0m fake_output \u001b[38;5;241m=\u001b[39m discriminator(fake_data)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Shapes of real and fake data must match."
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size_discriminator = 1710  # Input size should match the length of each segment for length and angle only\n",
    "hidden_size_discriminator1 = 512\n",
    "hidden_size_discriminator2 = 256\n",
    "hidden_size_discriminator3 = 128\n",
    "hidden_size_discriminator4 = 64\n",
    "output_size_discriminator = 1\n",
    "\n",
    "input_size_generator = 32\n",
    "hidden_size_generator1 = 64\n",
    "hidden_size_generator2 = 128\n",
    "hidden_size_generator3 = 256\n",
    "hidden_size_generator4 = 512\n",
    "output_size_generator = 1710  # Output size should match the size of each segment for length and angle only\n",
    "\n",
    "    \n",
    "discriminator = FNNDiscriminator(input_size_discriminator, hidden_size_discriminator1,\n",
    "                                  hidden_size_discriminator2, hidden_size_discriminator3,\n",
    "                                  hidden_size_discriminator4,\n",
    "                                  output_size_discriminator).to(device)\n",
    "generator = FNNGenerator(input_size_generator, hidden_size_generator1, hidden_size_generator2, hidden_size_generator3, hidden_size_generator4, output_size_generator).to(device)\n",
    "\n",
    "criterion = nn.BCELoss().to(device)  # Binary Cross-Entropy Loss\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.00005, betas=(0.95, 0.999))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.00001, betas=(0.95, 0.999))\n",
    "\n",
    "def gradient_penalty(discriminator, real_data, fake_data):\n",
    "    batch_size = real_data.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, 1).to(real_data.device)\n",
    "    epsilon = epsilon.expand_as(real_data)\n",
    "\n",
    "    interpolated = epsilon * real_data + (1 - epsilon) * fake_data\n",
    "    interpolated = torch.autograd.Variable(interpolated, requires_grad=True)\n",
    "\n",
    "    interpolated_prob = discriminator(interpolated)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=interpolated_prob,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones(interpolated_prob.size()).to(real_data.device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    return ((gradient_norm - 1) ** 2).mean()\n",
    "\n",
    "def wasserstein_loss(D_real, D_fake):\n",
    "    return D_fake.mean() - D_real.mean()\n",
    "\n",
    "def length_penalty_loss(flat_fake_data, batch_size):\n",
    "    lengths = flat_fake_data.view(batch_size, -1, 2)[:, :, 0]  # Extract length values\n",
    "    # Calculate absolute difference for each length from 0.5 and apply ReLU\n",
    "    penalty = torch.relu(torch.abs(lengths - 0.5))\n",
    "    # Sum the penalties\n",
    "    penalty = torch.sum(penalty)\n",
    "    return penalty\n",
    "\n",
    "def normalize_angles(angles, min_angle=-45, max_angle=90):\n",
    "    # Normalize angles to the range [-1, 1]\n",
    "    return 2 * ((angles - min_angle) / (max_angle - min_angle)) - 1\n",
    "\n",
    "def denormalize_angles(normalized_angles, min_angle=-45, max_angle=90):\n",
    "    # Denormalize angles to the range [min_angle, max_angle]\n",
    "    return ((normalized_angles + 1) / 2) * (max_angle - min_angle) + min_angle\n",
    "\n",
    "\n",
    "def sequential_penalty_loss(flat_fake_data, batch_size, min_allowed_diff=10):\n",
    "    angles = flat_fake_data.view(batch_size, -1, 2)[:, :, 1]\n",
    "    denormalized_angles = denormalize_angles(angles)\n",
    "    angle_diffs = denormalized_angles[:, 1:] - denormalized_angles[:, :-1]\n",
    "    penalty = torch.relu(min_allowed_diff - torch.abs(angle_diffs))\n",
    "    return torch.sum(penalty)\n",
    "\n",
    "def angle_loss(normalized_angles, min_allowed_diff_normalized=0.001, sequence_length=4):\n",
    "    \"\"\"\n",
    "    Compute the angle loss to penalize long sequences of small differences in normalized angles.\n",
    "    \n",
    "    Parameters:\n",
    "    normalized_angles (torch.Tensor): The tensor of normalized angles.\n",
    "    min_allowed_diff_normalized (float): The minimum allowed difference in normalized values.\n",
    "    sequence_length (int): The length of sequences to penalize if differences are below the min allowed difference.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The computed angle loss.\n",
    "    \"\"\"\n",
    "    # Compute the absolute differences between consecutive normalized angles\n",
    "    angle_diffs = torch.abs(normalized_angles[:, 1:] - normalized_angles[:, :-1])\n",
    "    \n",
    "    # Create mask for where the angle differences are below the minimum allowed deviation\n",
    "    small_diff_mask = angle_diffs < min_allowed_diff_normalized\n",
    "    \n",
    "    # Create a penalty mask for sequences of small differences\n",
    "    seq_mask = torch.nn.functional.conv1d(small_diff_mask.float().unsqueeze(1), \n",
    "                                          torch.ones((1, 1, sequence_length)).to(normalized_angles.device), \n",
    "                                          padding=sequence_length-1)\n",
    "    penalty_mask = (seq_mask >= sequence_length).float()\n",
    "    \n",
    "    # Sum up the penalties for all differences\n",
    "    total_penalty = torch.sum(penalty_mask)\n",
    "    \n",
    "    return total_penalty\n",
    "\n",
    "def balance_penalty_loss(normalized_angles):\n",
    "    # Count the number of positive and negative angles\n",
    "    positive_count = torch.sum(normalized_angles > 0.3333, dim=1)\n",
    "    negative_count = torch.sum(normalized_angles < 0.3333, dim=1)\n",
    "    \n",
    "    # Penalize the difference between positive and negative counts\n",
    "    imbalance_penalty = torch.abs(positive_count - negative_count)\n",
    "    \n",
    "    return torch.sum(imbalance_penalty)\n",
    "\n",
    "def angle_range_penalty_loss(normalized_fake_data):\n",
    "    angles = normalized_fake_data[:, 1::2]  # Extract normalized angles\n",
    "\n",
    "    # Penalize angles outside the range [0, 1]\n",
    "    penalty_outside_range = torch.relu(-angles) + torch.relu(angles - 1)\n",
    "    \n",
    "    # Sum the penalties\n",
    "    total_penalty = torch.sum(penalty_outside_range)\n",
    "\n",
    "    return total_penalty\n",
    "\n",
    "\n",
    "def length_difference_loss(real_lengths, fake_lengths):\n",
    "    real_avg_length = torch.mean(real_lengths)\n",
    "    fake_avg_length = torch.mean(fake_lengths)\n",
    "    return torch.abs(real_avg_length - fake_avg_length)\n",
    "\n",
    "def sequential_extreme_change_loss(flat_fake_data, batch_size):\n",
    "    angles = flat_fake_data.view(batch_size, -1, 2)[:, :, 1]\n",
    "    # The closer the angle is to -1 or 1, the higher the reward\n",
    "    penalty = torch.relu(-1 - angles) + torch.relu(angles - 1)\n",
    "    return torch.sum(penalty)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 350\n",
    "lambda_penalty = 3.0  # Weight for the length penalty loss\n",
    "lambda_difference = 10.0  # Weight for the length difference loss\n",
    "lambda_angle = 6.0  # Weight for the angle penalty loss\n",
    "lambda_angle_range = 15.0  # Weight for the angle range penalty loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for real_data in train_dataloader:\n",
    "        real_data = real_data.float().to(device).view(real_data.size(0), -1)  # Flatten and move to GPU\n",
    "\n",
    "        real_labels = torch.ones(len(real_data), 1).to(device)\n",
    "        fake_labels = torch.zeros(len(real_data), 1).to(device)\n",
    "\n",
    "        # Normalize angles in real data\n",
    "        normalized_real_data = real_data.clone()\n",
    "        normalized_real_data[:, 1::2] = normalize_angles(real_data[:, 1::2])\n",
    "\n",
    "        # Train discriminator\n",
    "        real_output = discriminator(normalized_real_data)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "        real_score = real_output\n",
    "\n",
    "        z = torch.randn(len(real_data), input_size_generator).unsqueeze(1).repeat(1, 855, 1).to(device)  # Generate random noise\n",
    "        fake_data = generator(z)\n",
    "        print(fake_data.shape)\n",
    "        print(normalized_real_data.shape)\n",
    "        fake_data = fake_data.view(fake_data.size(0), -1)  # Flatten the fake data\n",
    "        assert real_data.shape == fake_data.shape, \"Shapes of real and fake data must match.\"\n",
    "        print(fake_data.shape)\n",
    "        fake_output = discriminator(fake_data)\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "        fake_score = fake_output\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train generator\n",
    "        z = torch.randn(len(real_data), input_size_generator).unsqueeze(1).repeat(1, 855, 1).to(device)  # Generate random noise\n",
    "        fake_data = generator(z)\n",
    "        fake_data = fake_data.view(fake_data.size(0), -1)  # Flatten the fake data\n",
    "\n",
    "        output = discriminator(fake_data)\n",
    "\n",
    "        g_loss = criterion(output, real_labels)\n",
    "\n",
    "        # Add the length penalty loss\n",
    "        length_penalty = length_penalty_loss(fake_data, len(real_data))\n",
    "        \n",
    "        # Add the sequential penalty loss\n",
    "        sequential_penalty = sequential_penalty_loss(fake_data, len(real_data))\n",
    "\n",
    "        # Add the length difference loss\n",
    "        real_lengths = torch.norm(real_data.view(len(real_data), -1, 2), dim=2)  # Calculate lengths for real data\n",
    "        fake_lengths = torch.norm(fake_data.view(len(real_data), -1, 2), dim=2)  # Calculate lengths for fake data\n",
    "        difference_loss = length_difference_loss(real_lengths, fake_lengths)\n",
    "\n",
    "        # Add the angle penalty loss\n",
    "        fake_angles = fake_data[:, 1::2]  # Assuming angles are the second component\n",
    "        angle_penalty = angle_loss(fake_angles)\n",
    "\n",
    "        # Add the angle range penalty loss\n",
    "        angle_range_penalty = angle_range_penalty_loss(fake_data)\n",
    "        \n",
    "        # Add the balance penalty loss\n",
    "        balance_penalty = balance_penalty_loss(fake_angles)\n",
    "        \n",
    "        # Add the sequential extreme change loss\n",
    "        # sequential_penalty_extremes = sequential_extreme_change_loss(fake_data, len(real_data))\n",
    "\n",
    "        # Calculate the total generator loss\n",
    "        #total_g_loss = g_loss + lambda_angle_range * angle_range_penalty + lambda_penalty * sequential_penalty + length_penalty * lambda_penalty + balance_penalty * 2\n",
    "        total_g_loss = g_loss + length_penalty * lambda_penalty\n",
    "        total_g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {total_g_loss.item():.4f}, length_penalty: {length_penalty.item():.4f}, difference_loss: {difference_loss.item():.4f}, angle_penalty: {angle_penalty.item():.4f}, angle_range_penalty: {angle_range_penalty.item():.4f}, balance_penalty: {balance_penalty.item():.4f}, sequential_penalty: {sequential_penalty.item():.4f}')\n",
    "\n",
    "# Generate new data after training\n",
    "z = torch.randn(1, input_size_generator).unsqueeze(1).repeat(1, 855, 1).to(device)  # Generate random noise\n",
    "fake_data = generator(z)\n",
    "fake_data = fake_data.view(fake_data.size(0), 855, 2)\n",
    "\n",
    "generated_data = []\n",
    "\n",
    "for sample in fake_data:\n",
    "    segment_data = []\n",
    "    for vector in sample:\n",
    "        segment_data.append({\n",
    "            \"length\": vector[0].item(),\n",
    "            \"angleToNextVector\": vector[1].item()\n",
    "        })\n",
    "    generated_data.append({\"segments\": segment_data})\n",
    "\n",
    "print(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdf96068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized angles: tensor([0.1852, 0.2222, 0.2741, 0.6667, 0.3333])\n",
      "Denormalized angles: tensor([-20., -15.,  -8.,  45.,   0.])\n"
     ]
    }
   ],
   "source": [
    "def normalize_angles(angles, min_angle=-45, max_angle=90):\n",
    "    # Normalize angles to the range [0, 1]\n",
    "    normalized_angles = (angles - min_angle) / (max_angle - min_angle)\n",
    "    return normalized_angles\n",
    "\n",
    "def denormalize_angles(normalized_angles, min_angle=-45, max_angle=90):\n",
    "    # Denormalize angles from the range [0, 1] back to the original range\n",
    "    angles = normalized_angles * (max_angle - min_angle) + min_angle\n",
    "    return angles\n",
    "\n",
    "# Test the functions with the given angles\n",
    "angles = torch.tensor([-20, -15, -8, 45, 0], dtype=torch.float32)\n",
    "min_angle = -45\n",
    "max_angle = 90\n",
    "\n",
    "# Normalize the angles\n",
    "normalized_angles = normalize_angles(angles, min_angle, max_angle)\n",
    "print(\"Normalized angles:\", normalized_angles)\n",
    "\n",
    "# Denormalize the angles back to degrees\n",
    "denormalized_angles = denormalize_angles(normalized_angles, min_angle, max_angle)\n",
    "print(\"Denormalized angles:\", denormalized_angles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa56213b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# After training, generate new fake data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, input_size_generator)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m855\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Generate random noise\u001b[39;00m\n\u001b[0;32m      3\u001b[0m fake_data \u001b[38;5;241m=\u001b[39m generator(z)\n\u001b[0;32m      4\u001b[0m fake_data \u001b[38;5;241m=\u001b[39m fake_data\u001b[38;5;241m.\u001b[39mview(fake_data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m855\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# After training, generate new fake data\n",
    "z = torch.randn(1, input_size_generator).unsqueeze(1).repeat(1, 855, 1).to(device)  # Generate random noise\n",
    "fake_data = generator(z)\n",
    "fake_data = fake_data.view(fake_data.size(0), 855, 2)\n",
    "\n",
    "#fake_data[:, :, 1] = denormalize_angles(fake_data[:, :, 1])\n",
    "\n",
    "generated_data = []\n",
    "\n",
    "for sample in fake_data:\n",
    "    segment_data = []\n",
    "    for vector in sample:\n",
    "        segment_data.append({\n",
    "            \"length\": vector[0].item(),\n",
    "            \"angleToNextVector\": vector[1].item()\n",
    "        })\n",
    "    generated_data.append({\"segments\": segment_data})\n",
    "\n",
    "print(generated_data)\n",
    "\n",
    "def plot_segments_from_json(data):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for sample in data:\n",
    "        print(\"sample\", sample)\n",
    "        segments = sample[\"segments\"]\n",
    "        x, y = 0, 0  # Starting point\n",
    "        \n",
    "        for segment in segments:\n",
    "            length = segment[\"length\"]\n",
    "            tmpangle = segment[\"angleToNextVector\"]\n",
    "            while tmpangle < 0:\n",
    "                tmpangle = tmpangle + 360\n",
    "            angle = np.radians(segment[\"angleToNextVector\"])  # Convert angle to radians\n",
    "\n",
    "            new_x = x + length * np.cos(angle)\n",
    "            new_y = y + length * np.sin(angle) * 100\n",
    "            segment[\"x\"] = new_x\n",
    "            segment[\"y\"] = new_x\n",
    "            \n",
    "            # Plot a line segment from (x, y) to (new_x, new_y)\n",
    "            ax.plot([x, new_x], [y, new_y], 'bo-', markersize=5)  # 'bo-' for blue line with circle markers\n",
    "\n",
    "            # Update the current point\n",
    "            x, y = new_x, new_y\n",
    "\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.autoscale()\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title('Segments Plot')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_segments_from_json(generated_data)\n",
    "\n",
    "#read ground-1.json and plot the segment\n",
    "\n",
    "with open('../learning-data/ground-1.json', 'r') as f:\n",
    "    temp_ground_data = json.load(f)\n",
    "    temp_segments = temp_ground_data['segments']\n",
    "    plot_segments_from_json([{\"segments\": temp_segments}])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
