{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d97c5ff2-2086-4101-ac95-de69aeb086d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import winsound\n",
    "\n",
    "\n",
    "random_seed = 1337\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "# torch.use_deterministic_algorithms(False)\n",
    "train_ratio = 0.80 # 80% for training, 20% for validation\n",
    "\n",
    "BATCH_SIZE= 64\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "NUM_WORKERS=int(os.cpu_count() / 2) \n",
    "\n",
    "print(NUM_WORKERS)\n",
    "print(AVAIL_GPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aeaab97",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(failed_file)\n\u001b[0;32m     20\u001b[0m     failed_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m         ground_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32mc:\\Users\\edine\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen codecs>:260\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "json_dir = '../learning-data/'\n",
    "\n",
    "all_segments = []\n",
    "all_grass_positions = []\n",
    "all_starting_points = []\n",
    "\n",
    "# Helper function to compute length and angle\n",
    "def compute_length_and_angle(x1, y1, x2, y2):\n",
    "    length = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "    angle = np.arctan2(y2 - y1, x2 - x1)\n",
    "    return length, angle\n",
    "\n",
    "failed_file = None\n",
    "# Iterate through JSON files\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        # delete failed file if it exists\n",
    "        if failed_file is not None:\n",
    "            os.remove(failed_file)\n",
    "            failed_file = None\n",
    "        with open(os.path.join(json_dir, filename), 'r') as f:\n",
    "            try:\n",
    "                ground_data = json.load(f)\n",
    "                segments = ground_data['segments']\n",
    "                grass_positions = ground_data['grassPositions']\n",
    "                # check if ground data contains starting point\n",
    "                if 'startingPoint' in ground_data:\n",
    "                    starting_point = ground_data['startingPoint']\n",
    "                else:\n",
    "                    starting_point = all_starting_points[-1]\n",
    "\n",
    "                # Append data to lists\n",
    "                all_segments.append(segments)\n",
    "                all_grass_positions.append(grass_positions)\n",
    "                all_starting_points.append(starting_point)\n",
    "            except:\n",
    "                print(f'Error reading {filename}')\n",
    "                # delete the file\n",
    "                failed_file = os.path.join(json_dir, filename)\n",
    "                continue\n",
    "\n",
    "print(f'Number of segments: {len(all_segments)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the data and save it to a file for later use\n",
    "with open('../learning-data/ground_data.pkl', 'wb') as f:\n",
    "    pickle.dump((all_segments, all_grass_positions, all_starting_points), f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419989cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the serialized data\n",
    "with open('../learning-data/ground_data.pkl', 'rb') as f:\n",
    "    all_segments, all_grass_positions, all_starting_points = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_list = []\n",
    "print(all_segments.__len__())\n",
    "for segment in all_segments:\n",
    "    #segment_tensor = torch.tensor([[point['length'], point['angleToNextVector'], point['x'], point['y']] for point in segment], dtype=torch.float32)\n",
    "    #segment_tensor = torch.tensor([[point['x'], point['y']] for point in segment], dtype=torch.float32)\n",
    "    segment_tensor = torch.tensor([[point['length'], point['angleToNextVector']] for point in segment], dtype=torch.float32)\n",
    "    segments_list.append(segment_tensor)\n",
    "segments_tensor = torch.stack(segments_list)\n",
    "\n",
    "print(segments_tensor.shape)\n",
    "\n",
    "# Convert grass positions to tensor\n",
    "grass_positions_tensor = torch.tensor(all_grass_positions, dtype=torch.int64)  # Assuming grass positions are integers\n",
    "\n",
    "# Convert starting points to tensor\n",
    "starting_points_tensor = torch.tensor(all_starting_points, dtype=torch.float32)\n",
    "\n",
    "# Serialize the tensors\n",
    "with open('../learning-data/ground_tensors.pkl', 'wb') as f:\n",
    "    pickle.dump((segments_tensor, grass_positions_tensor, starting_points_tensor), f)\n",
    "\n",
    "#Serialize the tensors with coordinates    \n",
    "#with open('../learning-data/ground_tensors_with_coords.pkl', 'wb') as f:\n",
    "    #pickle.dump((segments_tensor, grass_positions_tensor, starting_points_tensor), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941c827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([102315, 855, 2])\n",
      "torch.Size([102315, 1])\n"
     ]
    }
   ],
   "source": [
    "# Load serialized tensors without coordinates\n",
    "with open('../learning-data/ground_tensors.pkl', 'rb') as f:\n",
    "    segments_tensor, grass_positions_tensor, starting_points_tensor = pickle.load(f)\n",
    "    \n",
    "with open('../learning-data/conditions_special.pkl', 'rb') as f:\n",
    "    conditions_tensor = pickle.load(f)\n",
    "    \n",
    "print(segments_tensor.shape)\n",
    "print(conditions_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the serialized tensors\n",
    "with open('../learning-data/ground_tensors_with_coords.pkl', 'rb') as f:\n",
    "    segments_tensor, grass_positions_tensor, starting_points_tensor = pickle.load(f)\n",
    "    \n",
    "batch_size = segments_tensor.shape[0]\n",
    "\n",
    "# Extract angles and y-coordinates from segments_tensor\n",
    "angles = segments_tensor[:, :, 1]\n",
    "y_coords = segments_tensor[:, :, 3]\n",
    "\n",
    "# Calculate angle metrics\n",
    "mean_angle = angles.mean(dim=1, keepdim=True)\n",
    "max_angle = angles.max(dim=1, keepdim=True)[0]\n",
    "min_angle = angles.min(dim=1, keepdim=True)[0]\n",
    "std_angle = angles.std(dim=1, keepdim=True)\n",
    "sum_abs_diff_angles = torch.sum(torch.abs(angles[:, 1:] - angles[:, :-1]), dim=1, keepdim=True)\n",
    "\n",
    "# Calculate y-coordinate metrics\n",
    "mean_y_coord = y_coords.mean(dim=1, keepdim=True)\n",
    "\n",
    "# Function to normalize a tensor and handle potential NaNs\n",
    "def normalize(tensor):\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    if max_val - min_val == 0: \n",
    "        return torch.zeros_like(tensor)\n",
    "    epsilon = 1e-8\n",
    "    normalized_tensor = (tensor - min_val) / (max_val - min_val + epsilon)\n",
    "    return normalized_tensor\n",
    "\n",
    "# Normalize each metric separately\n",
    "mean_angle_normalized = normalize(mean_angle)\n",
    "max_angle_normalized = normalize(max_angle)\n",
    "min_angle_normalized = normalize(min_angle)\n",
    "std_angle_normalized = normalize(std_angle)\n",
    "sum_abs_diff_angles_normalized = normalize(sum_abs_diff_angles)\n",
    "mean_y_coord_normalized = normalize(mean_y_coord)\n",
    "\n",
    "# Check for NaNs in normalized metrics\n",
    "print(torch.isnan(mean_angle_normalized).any())\n",
    "print(torch.isnan(max_angle_normalized).any())\n",
    "print(torch.isnan(min_angle_normalized).any())\n",
    "print(torch.isnan(std_angle_normalized).any())\n",
    "print(torch.isnan(sum_abs_diff_angles_normalized).any())\n",
    "print(torch.isnan(mean_y_coord_normalized).any())\n",
    "\n",
    "# Composite condition tensor\n",
    "conditions_tensor = (mean_angle_normalized + max_angle_normalized + min_angle_normalized + std_angle_normalized + sum_abs_diff_angles_normalized + mean_y_coord_normalized) / 6\n",
    "\n",
    "# Ensure conditions_tensor is [batch_size, 1]\n",
    "conditions_tensor = conditions_tensor.view(batch_size, 1)\n",
    "\n",
    "# Check the conditions tensor\n",
    "print(conditions_tensor)\n",
    "\n",
    "print(conditions_tensor.shape)\n",
    "print(conditions_tensor.max())\n",
    "print(conditions_tensor.min())\n",
    "print(conditions_tensor.mean())\n",
    "\n",
    "# Serialize the conditions tensor\n",
    "with open('../learning-data/conditions_special.pkl', 'wb') as f:\n",
    "    pickle.dump(conditions_tensor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2646f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 0: torch.Size([855, 2]), Condition 0: torch.Size([1])\n",
      "Data 1: torch.Size([855, 2]), Condition 1: torch.Size([1])\n",
      "Data 2: torch.Size([855, 2]), Condition 2: torch.Size([1])\n",
      "Data 3: torch.Size([855, 2]), Condition 3: torch.Size([1])\n",
      "Data 4: torch.Size([855, 2]), Condition 4: torch.Size([1])\n",
      "Training dataset size: 81852\n",
      "Validation dataset size: 20463\n",
      "Segments tensor shape: torch.Size([102315, 855, 2])\n",
      "Conditions tensor shape: torch.Size([102315, 1])\n",
      "Max value in normalized conditions tensor: tensor(1.)\n",
      "Min value in normalized conditions tensor: tensor(0.)\n",
      "Mean value in normalized conditions tensor: tensor(0.4929)\n",
      "Segments tensor length: 102315\n",
      "Conditions tensor length: 102315\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "def normalize_conditions(conditions):\n",
    "    # Normalize each feature (column) independently across the entire dataset\n",
    "    min_val = conditions.min(dim=0, keepdim=True)[0] \n",
    "    max_val = conditions.max(dim=0, keepdim=True)[0]\n",
    "    normalized_conditions = (conditions - min_val) / (max_val - min_val + 1e-8)  # Add epsilon to avoid division by zero\n",
    "    return normalized_conditions, min_val, max_val\n",
    "\n",
    "def denormalize_conditions(normalized_conditions, min_val, max_val):\n",
    "    # Denormalize using the min and max values for each feature\n",
    "    conditions = normalized_conditions * (max_val - min_val + 1e-8) + min_val\n",
    "    return conditions\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, conditions, min_val=None, max_val=None):\n",
    "        assert len(data) == len(conditions), \"Data and conditions must have the same length.\"\n",
    "        self.data = data\n",
    "        \n",
    "        if min_val is None or max_val is None:\n",
    "            self.conditions, self.min_val, self.max_val = normalize_conditions(conditions)\n",
    "        else:\n",
    "            self.min_val = min_val\n",
    "            self.max_val = max_val\n",
    "            self.conditions = (conditions - self.min_val) / (self.max_val - self.min_val + 1e-8)  # Add epsilon here as well\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        condition = self.conditions[idx]\n",
    "        return data, condition\n",
    "\n",
    "# Normalize conditions\n",
    "normalized_conditions, min_val, max_val = normalize_conditions(conditions_tensor)\n",
    "\n",
    "# Create the dataset with the normalized conditions\n",
    "dataset = CustomDataset(segments_tensor, normalized_conditions, min_val=min_val, max_val=max_val)\n",
    "\n",
    "# Check the first few items in the dataset\n",
    "for i in range(5):\n",
    "    data, condition = dataset[i]\n",
    "    print(f\"Data {i}: {data.shape}, Condition {i}: {condition.shape}\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_ratio = 0.8  # or your desired train/val split ratio\n",
    "BATCH_SIZE = 64    # or your desired batch size\n",
    "train_size = int(len(dataset) * train_ratio)\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances for training and validation sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Optionally, you can print the sizes of the datasets\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "\n",
    "# Debugging output\n",
    "print(\"Segments tensor shape:\", segments_tensor.shape)\n",
    "print(\"Conditions tensor shape:\", conditions_tensor.shape)\n",
    "print(\"Max value in normalized conditions tensor:\", normalized_conditions.max())\n",
    "print(\"Min value in normalized conditions tensor:\", normalized_conditions.min())\n",
    "print(\"Mean value in normalized conditions tensor:\", normalized_conditions.mean())\n",
    "print(\"Segments tensor length:\", len(segments_tensor))\n",
    "print(\"Conditions tensor length:\", len(conditions_tensor))\n",
    "\n",
    "# Check if DataLoader is working correctl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d048c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int((len(segments_tensor) * train_ratio) )# % for training\n",
    "val_size = int(len(segments_tensor) - train_size)  # Remaining % for validation\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_dataset, val_dataset = random_split(segments_tensor, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances for training and validation sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Optionally, you can print the sizes of the datasets\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1519101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model with dropout\n",
    "class FNNDiscriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, hidden_size4, output_size, dropout_prob=0.15):\n",
    "        super(FNNDiscriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc4 = nn.Linear(hidden_size3, hidden_size4)\n",
    "        self.fc5 = nn.Linear(hidden_size4, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.dropout(self.relu(self.fc3(x)))\n",
    "        x = self.dropout(self.relu(self.fc4(x)))\n",
    "        x = self.sigmoid(self.fc5(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Assuming input_size_discriminator = 1710\n",
    "    \n",
    "class FNNGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, hidden_size4, hidden_size5, hidden_size6, hidden_size7, output_size, dropout_prob=0.255):\n",
    "        super(FNNGenerator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc4 = nn.Linear(hidden_size3, hidden_size4)\n",
    "        self.fc5 = nn.Linear(hidden_size4, hidden_size5)\n",
    "        self.fc6 = nn.Linear(hidden_size5, hidden_size6)\n",
    "        self.fc7 = nn.Linear(hidden_size6, hidden_size7)\n",
    "        self.fc8 = nn.Linear(hidden_size7, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.dropout(self.relu(self.fc3(x)))\n",
    "        x = self.dropout(self.relu(self.fc4(x)))\n",
    "        x = self.dropout(self.relu(self.fc5(x)))\n",
    "        x = self.dropout(self.relu(self.fc6(x)))\n",
    "        x = self.dropout(self.relu(self.fc7(x)))\n",
    "        x = self.tanh(self.fc8(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039bf469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATEST NON CONDITIONAL\n",
    "class FNNGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_prob=0.115):\n",
    "        super(FNNGenerator, self).__init__()\n",
    "        layers = []\n",
    "        sizes = [input_size] + hidden_sizes\n",
    "        \n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))  # Fully connected layer\n",
    "            layers.append(nn.BatchNorm1d(sizes[i+1]))  # Batch normalization\n",
    "            layers.append(nn.LeakyReLU(0.1, inplace=True))  # Leaky ReLU activation with in-place operation\n",
    "            layers.append(nn.Dropout(dropout_prob))  # Dropout for regularization\n",
    "        \n",
    "        layers.append(nn.Linear(sizes[-1], output_size))  # Final layer to output the generated data\n",
    "        layers.append(nn.Tanh())  # Tanh activation to keep the output within a certain range\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "class FNNDiscriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_prob=0.2):\n",
    "        super(FNNDiscriminator, self).__init__()\n",
    "        layers = []\n",
    "        sizes = [input_size] + hidden_sizes\n",
    "        \n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))  # Fully connected layer\n",
    "            layers.append(nn.BatchNorm1d(sizes[i+1]))  # Batch normalization\n",
    "            layers.append(nn.ReLU(inplace=True))  # ReLU activation\n",
    "            layers.append(nn.Dropout(dropout_prob))  # Dropout for regularization\n",
    "        \n",
    "        layers.append(nn.Linear(sizes[-1], output_size))  # Final layer to output a single value\n",
    "        layers.append(nn.Sigmoid())  # Sigmoid activation for binary classification\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b36f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATEST CONDITIONAL\n",
    "class FNNGenerator(nn.Module):\n",
    "    def __init__(self, input_size, condition_size, hidden_sizes, output_size, dropout_prob=0.0):\n",
    "        super(FNNGenerator, self).__init__()\n",
    "        layers = []\n",
    "        sizes = [input_size + condition_size] + hidden_sizes  # Add condition size to the input size\n",
    "        \n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))  # Fully connected layer\n",
    "            layers.append(nn.BatchNorm1d(sizes[i+1]))  # Batch normalization\n",
    "            layers.append(nn.LeakyReLU(0.1, inplace=True))  # Leaky ReLU activation with in-place operation\n",
    "            # layers.append(nn.Dropout(dropout_prob))  # Dropout for regularization\n",
    "        \n",
    "        layers.append(nn.Linear(sizes[-1], output_size))  # Final layer to output the generated data\n",
    "        layers.append(nn.Tanh())  # Tanh activation to keep the output within a certain range\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        condition = condition.expand(x.size(0), -1)  # Ensure conditions have correct shape\n",
    "        x = torch.cat((x, condition), dim=1)  # Concatenate noise with condition\n",
    "        return self.model(x)\n",
    "    \n",
    "class FNNDiscriminator(nn.Module):\n",
    "    def __init__(self, input_size, condition_size, hidden_sizes, output_size, dropout_prob=0.255):\n",
    "        super(FNNDiscriminator, self).__init__()\n",
    "        layers = []\n",
    "        sizes = [input_size + condition_size] + hidden_sizes  # Add condition size to the input size\n",
    "        \n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))  # Fully connected layer\n",
    "            layers.append(nn.BatchNorm1d(sizes[i+1]))  # Batch normalization\n",
    "            layers.append(nn.ReLU(inplace=True))  # ReLU activation\n",
    "            layers.append(nn.Dropout(dropout_prob))  # Dropout for regularization\n",
    "        \n",
    "        layers.append(nn.Linear(sizes[-1], output_size))  # Final layer to output a single value\n",
    "        layers.append(nn.Sigmoid())  # Sigmoid activation for binary classification\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        condition = condition.expand(x.size(0), -1)  # Ensure conditions have correct shape\n",
    "        x = torch.cat((x, condition), dim=1)  # Concatenate real/fake data with condition\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- NON CONDITIONAL -----------------------------\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "# input_size_discriminator = 1710  # Input size should match the length of each segment for length and angle only\n",
    "# hidden_size_discriminator1 = 256\n",
    "# hidden_size_discriminator2 = 128\n",
    "# hidden_size_discriminator3 = 32\n",
    "# hidden_size_discriminator4 = 16\n",
    "# output_size_discriminator = 1\n",
    "\n",
    "# input_size_generator = 16\n",
    "# hidden_size_generator1 = 64\n",
    "# hidden_size_generator2 = 256\n",
    "# hidden_size_generator3 = 256\n",
    "# hidden_size_generator4 = 512\n",
    "# hidden_size_generator5 = 512\n",
    "# hidden_size_generator6 = 1024\n",
    "# hidden_size_generator7 = 256\n",
    "# output_size_generator = 1710  # Output size should match the size of each segment for length and angle only\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size_discriminator = 1710  # Input size should match the length of each segment for length and angle only\n",
    "hidden_sizes_discriminator = [256, 256, 256]\n",
    "output_size_discriminator = 1\n",
    "\n",
    "input_size_generator = 16\n",
    "hidden_sizes_generator = [256, 256, 256]\n",
    "output_size_generator = 1710  # Output size should match the size of each segment for length and angle only\n",
    "\n",
    "condition_size = 1\n",
    "\n",
    "    \n",
    "# discriminator = FNNDiscriminator(input_size_discriminator, hidden_size_discriminator1,\n",
    "#                                   hidden_size_discriminator2, hidden_size_discriminator3,\n",
    "#                                   hidden_size_discriminator4,\n",
    "#                                   output_size_discriminator).to(device)\n",
    "# generator = FNNGenerator(input_size_generator, hidden_size_generator1, hidden_size_generator2, hidden_size_generator3, hidden_size_generator4, hidden_size_generator5, hidden_size_generator6, hidden_size_generator7, output_size_generator).to(device)\n",
    "\n",
    "#### NON CONDITIONAL\n",
    "discriminator = FNNDiscriminator(input_size_discriminator, hidden_sizes_discriminator, output_size_discriminator).to(device)\n",
    "generator = FNNGenerator(input_size_generator, hidden_sizes_generator, output_size_generator).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss().to(device)  # Binary Cross-Entropy Loss\n",
    "#d_optimizer = optim.Adam(discriminator.parameters(), lr=0.000001, betas=(0.95, 0.999))\n",
    "#g_optimizer = optim.Adam(generator.parameters(), lr=0.00001, betas=(0.65, 0.999))\n",
    "#d_optimizer = optim.Adam(discriminator.parameters(), lr=0.00001)\n",
    "# g_optimizer = optim.Adagrad(generator.parameters(), lr=0.000025)\n",
    "\n",
    "d_optimizer = optim.AdamW(discriminator.parameters(), lr=0.00005, betas=(0.75, 0.999), weight_decay=1e-2)\n",
    "g_optimizer = optim.AdamW(generator.parameters(), lr=0.00001, betas=(0.75, 0.999), weight_decay=1e-2)\n",
    "\n",
    "# d_optimizer = optim.AdamW(discriminator.parameters(), lr=0.00005698788315, betas=(0.95, 0.999), weight_decay=1e-2)\n",
    "# g_optimizer = optim.AdamW(generator.parameters(), lr=0.0000164885, betas=(0.95, 0.999), weight_decay=1e-2)\n",
    "\n",
    "# scheduler_g = optim.lr_scheduler.StepLR(g_optimizer, step_size=8, gamma=0.01)\n",
    "# scheduler_d = optim.lr_scheduler.StepLR(d_optimizer, step_size=8, gamma=0.01)\n",
    "\n",
    "def gradient_penalty(discriminator, real_data, fake_data):\n",
    "    batch_size = real_data.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, 1).to(real_data.device)\n",
    "    epsilon = epsilon.expand_as(real_data)\n",
    "\n",
    "    interpolated = epsilon * real_data + (1 - epsilon) * fake_data\n",
    "    interpolated = torch.autograd.Variable(interpolated, requires_grad=True)\n",
    "\n",
    "    interpolated_prob = discriminator(interpolated)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=interpolated_prob,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones(interpolated_prob.size()).to(real_data.device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    return ((gradient_norm - 1) ** 2).mean()\n",
    "\n",
    "def wasserstein_loss(D_real, D_fake):\n",
    "    return D_fake.mean() - D_real.mean()\n",
    "\n",
    "def length_penalty_loss(flat_fake_data, batch_size):\n",
    "    lengths = flat_fake_data.view(batch_size, -1, 2)[:, :, 0]  # Extract length values\n",
    "    # Calculate absolute difference for each length from 0.5 and apply ReLU\n",
    "    penalty = torch.relu(torch.abs(lengths - 0.5))\n",
    "    # Sum the penalties\n",
    "    penalty = torch.sum(penalty)\n",
    "    return penalty\n",
    "\n",
    "def normalize_angles(angles, min_angle=-45, max_angle=60):\n",
    "    # Normalize angles to the range [-1, 1]\n",
    "    return 2 * ((angles - min_angle) / (max_angle - min_angle)) - 1\n",
    "\n",
    "def denormalize_angles(normalized_angles, min_angle=-45, max_angle=60):\n",
    "    # Denormalize angles to the range [min_angle, max_angle]\n",
    "    return ((normalized_angles + 1) / 2) * (max_angle - min_angle) + min_angle\n",
    "\n",
    "\n",
    "\n",
    "# def sequential_penalty_loss(flat_fake_data, batch_size, min_allowed_diff=10):\n",
    "#     angles = flat_fake_data.view(batch_size, -1, 2)[:, :, 1]\n",
    "#     denormalized_angles = denormalize_angles(angles)\n",
    "#     angle_diffs = denormalized_angles[:, 1:] - denormalized_angles[:, :-1]\n",
    "#     penalty = torch.relu(min_allowed_diff - torch.abs(angle_diffs))\n",
    "#     return torch.sum(penalty)\n",
    "\n",
    "def sequential_penalty_loss(flat_fake_data, batch_size, min_allowed_diff=10, min_angle=-45, max_angle=60):\n",
    "    angles = flat_fake_data.view(batch_size, -1, 2)[:, :, 1]\n",
    "    denormalized_angles = denormalize_angles(angles, min_angle, max_angle)\n",
    "    \n",
    "    # Calculate angle differences and account for circular nature\n",
    "    angle_diffs = denormalized_angles[:, 1:] - denormalized_angles[:, :-1]\n",
    "    angle_diffs = (angle_diffs + 180) % 360 - 180  # Convert to range [-180, 180]\n",
    "    \n",
    "    # Calculate penalties\n",
    "    penalty = torch.relu(torch.abs(angle_diffs) - min_allowed_diff)\n",
    "    return torch.sum(penalty)\n",
    "\n",
    "def angle_loss(normalized_angles, min_allowed_diff_normalized=0.001, sequence_length=4):\n",
    "    \"\"\"\n",
    "    Compute the angle loss to penalize long sequences of small differences in normalized angles.\n",
    "    \n",
    "    Parameters:\n",
    "    normalized_angles (torch.Tensor): The tensor of normalized angles.\n",
    "    min_allowed_diff_normalized (float): The minimum allowed difference in normalized values.\n",
    "    sequence_length (int): The length of sequences to penalize if differences are below the min allowed difference.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The computed angle loss.\n",
    "    \"\"\"\n",
    "    # Compute the absolute differences between consecutive normalized angles\n",
    "    angle_diffs = torch.abs(normalized_angles[:, 1:] - normalized_angles[:, :-1])\n",
    "    \n",
    "    # Create mask for where the angle differences are below the minimum allowed deviation\n",
    "    small_diff_mask = angle_diffs < min_allowed_diff_normalized\n",
    "    \n",
    "    # Create a penalty mask for sequences of small differences\n",
    "    seq_mask = torch.nn.functional.conv1d(small_diff_mask.float().unsqueeze(1), \n",
    "                                          torch.ones((1, 1, sequence_length)).to(normalized_angles.device), \n",
    "                                          padding=sequence_length-1)\n",
    "    penalty_mask = (seq_mask >= sequence_length).float()\n",
    "    \n",
    "    # Sum up the penalties for all differences\n",
    "    total_penalty = torch.sum(penalty_mask)\n",
    "    \n",
    "    return total_penalty\n",
    "\n",
    "def balance_penalty_loss(normalized_angles):\n",
    "    # Count the number of positive and negative angles\n",
    "    positive_count = torch.sum(normalized_angles > -0.1428, dim=1) * 1.835\n",
    "    negative_count = torch.sum(normalized_angles < -0.1428, dim=1)\n",
    "    \n",
    "    # Penalize the difference between positive and negative counts\n",
    "    imbalance_penalty = torch.abs(positive_count - negative_count)\n",
    "    \n",
    "    return torch.sum(imbalance_penalty)\n",
    "\n",
    "def angle_range_penalty_loss(normalized_fake_data):\n",
    "    angles = normalized_fake_data[:, 1::2]  # Extract angle values\n",
    "    # Penalize angles outside the range [-1, 1]\n",
    "    penalty_outside_range = torch.relu(-angles) + torch.relu(angles - 1)\n",
    "    # Sum the penalties\n",
    "    total_penalty = torch.sum(penalty_outside_range)\n",
    "    return total_penalty\n",
    "\n",
    "def y_range_penalty_loss(fake_data, batch_size):\n",
    "    x,y = calculate_cumulative_coordinates_parallel(fake_data , batch_size)\n",
    "    \n",
    "    # Penalize y values outside the range [2, 25]\n",
    "    penalty_outside_range = torch.relu(-y) + torch.relu(y - 25)\n",
    "    \n",
    "    # Sum the penalties\n",
    "    total_penalty = torch.sum(penalty_outside_range)\n",
    "    \n",
    "    return total_penalty\n",
    "\n",
    "def x_range_penalty_loss(fake_data, batch_size):\n",
    "    x, y = calculate_cumulative_coordinates_parallel(fake_data, batch_size)\n",
    "    \n",
    "    # Take the last x-coordinate and penalize if it's less than 425\n",
    "    penalty_outside_range = torch.relu(425 - x[:, -1])\n",
    "    \n",
    "    # Compute the maximum penalty across the batch\n",
    "    total_penalty = torch.max(penalty_outside_range)\n",
    "    \n",
    "    return total_penalty\n",
    "    \n",
    "\n",
    "def calculate_cumulative_coordinates_parallel(flat_fake_data, batch_size):\n",
    "    angles = flat_fake_data.view(batch_size, -1, 2)[:, :, 1]\n",
    "    lengths = flat_fake_data.view(batch_size, -1, 2)[:, :, 0]\n",
    "    \n",
    "    # denormalize angles\n",
    "    angles = denormalize_angles(angles)\n",
    "    \n",
    "    # Convert angles from degrees to radians\n",
    "    angles = angles * (torch.pi / 180.0)\n",
    "    \n",
    "    # Calculate the delta x and y for each segment\n",
    "    delta_x = lengths * torch.cos(angles)\n",
    "    delta_y = lengths * torch.sin(angles)\n",
    "    \n",
    "    # Use cumulative sum to get the coordinates\n",
    "    x = torch.cumsum(delta_x, dim=1)\n",
    "    y = torch.cumsum(delta_y, dim=1)\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "\n",
    "\n",
    "def length_difference_loss(real_lengths, fake_lengths):\n",
    "    real_avg_length = torch.mean(real_lengths)\n",
    "    fake_avg_length = torch.mean(fake_lengths)\n",
    "    return torch.abs(real_avg_length - fake_avg_length)\n",
    "\n",
    "def sequential_extreme_change_loss(flat_fake_data, batch_size, min_angle=-45, max_angle=60, max_diff=20):\n",
    "    angles = flat_fake_data.view(batch_size, -1, 2)[:, :, 1]\n",
    "    \n",
    "    # Denormalize angles to their actual range\n",
    "    denormalized_angles = denormalize_angles(angles, min_angle, max_angle)\n",
    "    \n",
    "    # Compute the differences between consecutive angles\n",
    "    angle_diffs = torch.abs(denormalized_angles[:, 1:] - denormalized_angles[:, :-1])\n",
    "    \n",
    "    # Calculate the penalty for each angle difference\n",
    "    penalties = torch.relu(angle_diffs - max_diff)\n",
    "    \n",
    "    # Sum up the penalties for all differences\n",
    "    total_penalty = torch.sum(penalties)\n",
    "    \n",
    "    return total_penalty\n",
    "\n",
    "\n",
    "# def sequential_extreme_change_loss(flat_fake_data, batch_size):\n",
    "#     angles = flat_fake_data.view(batch_size, -1, 2)[:, :, 1]\n",
    "    \n",
    "#     # Compute the absolute differences between consecutive angles\n",
    "#     angle_diffs = torch.abs(angles[:, 1:] - angles[:, :-1])\n",
    "    \n",
    "#     # Create mask for where the angle differences are above the maximum allowed deviation which is 20 degrees normalized between -1 and 1 on a scale of -45 to 60\n",
    "#     normalized_max_diff = 0.3333  # 20 degrees normalized between -1 and 1\n",
    "#     extreme_change_mask = angle_diffs > normalized_max_diff\n",
    "    \n",
    "#     # Create a penalty mask for sequences of extreme changes\n",
    "#     seq_mask = torch.nn.functional.conv1d(extreme_change_mask.float().unsqueeze(1),\n",
    "#                                              torch.ones((1, 1, 3)).to(angles.device),\n",
    "#                                              padding=2)\n",
    "    \n",
    "#     penalty_mask = (seq_mask >= 2).float() # 2 consecutive extreme changes\n",
    "    \n",
    "#     # Sum up the penalties for all differences\n",
    "#     total_penalty = torch.sum(penalty_mask)\n",
    "    \n",
    "#     return total_penalty\n",
    "\n",
    "# Initialize lists to store losses for plotting\n",
    "train_d_losses = []\n",
    "train_g_losses = []\n",
    "val_d_losses = []\n",
    "val_g_losses = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 180\n",
    "lambda_penalty = 3.0  # Weight for the length penalty loss\n",
    "lambda_difference = 10.0  # Weight for the length difference loss\n",
    "lambda_angle = 6.0  # Weight for the angle penalty loss\n",
    "lambda_angle_range = 15.0  # Weight for the angle range penalty loss\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience = 8\n",
    "patience_counter = 0\n",
    "    #--------------------------------------------------------- CONDITIONAL ---------------------------------------------------------\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    discriminator.train()  # Set discriminator to training mode\n",
    "    generator.train()  # Set generator to training mode\n",
    "    \n",
    "    total_d_loss = 0.0\n",
    "    total_g_loss = 0.0\n",
    "    num_batches = 0\n",
    "    correct_real = 0\n",
    "    correct_fake = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for i, real_data in enumerate(train_dataloader):\n",
    "        num_batches += 1\n",
    "        real_data = real_data.float().to(device).view(real_data.size(0), -1)  # Flatten and move to GPU\n",
    "        real_labels = torch.ones(len(real_data), 1).to(device) * 0.9\n",
    "        fake_labels = torch.zeros(len(real_data), 1).to(device) + 0.1\n",
    "        normalized_real_data = real_data.clone()\n",
    "        normalized_real_data[:, 1::2] = normalize_angles(real_data[:, 1::2])\n",
    "        \n",
    "        \n",
    "        # Noise injection to real data (out-of-place)\n",
    "        noisy_real_data = real_data + torch.normal(0, 0.05, real_data.shape).to(device)\n",
    "        \n",
    "        # Train discriminator\n",
    "        real_output = discriminator(normalized_real_data)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "        z = torch.randn(len(real_data), input_size_generator).to(device)  # Generate random noise\n",
    "        fake_data = generator(z).view(z.size(0), -1)  # Flatten the fake data\n",
    "\n",
    "        # Noise injection to fake data (out-of-place)\n",
    "        noisy_fake_data = fake_data + torch.normal(0, 0.65, fake_data.shape).to(device)\n",
    "        \n",
    "        fake_output = discriminator(noisy_fake_data)\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        total_d_loss += d_loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct_real += (real_output > 0.5).sum().item()\n",
    "        correct_fake += (fake_output < 0.5).sum().item()\n",
    "        total_samples += len(real_data)\n",
    "\n",
    "        # Train generator every 5 steps\n",
    "        if i % 2 == 0:\n",
    "            z = torch.randn(len(real_data), input_size_generator).to(device)\n",
    "            fake_data = generator(z).view(z.size(0), -1)\n",
    "\n",
    "            # Noise injection to fake data (out-of-place)\n",
    "            noisy_fake_data = fake_data + torch.normal(0, 0.65, fake_data.shape).to(device)\n",
    "\n",
    "            output = discriminator(noisy_fake_data)\n",
    "            g_loss = criterion(output, real_labels)\n",
    "            length_penalty = length_penalty_loss(fake_data, len(real_data))\n",
    "            \n",
    "            # sequential_penalty = sequential_penalty_loss(fake_data, len(real_data))\n",
    "            real_lengths = torch.norm(real_data.view(len(real_data), -1, 2), dim=2)\n",
    "            fake_lengths = torch.norm(fake_data.view(len(fake_data), -1, 2), dim=2)\n",
    "            # difference_loss = length_difference_loss(real_lengths, fake_lengths)\n",
    "            fake_angles = fake_data[:, 1::2]\n",
    "            # angle_penalty = angle_loss(fake_angles)\n",
    "            # angle_range_penalty = angle_range_penalty_loss(fake_data)\n",
    "            balance_penalty = balance_penalty_loss(fake_angles)\n",
    "            sequential_extreme_change_loss_penalty = sequential_extreme_change_loss(fake_data, len(real_data))\n",
    "            \n",
    "            # Calculate sequential penalty\n",
    "            sequential_penalty = sequential_penalty_loss(fake_data, len(real_data))\n",
    "            \n",
    "            # Calculate y range penalty\n",
    "            y_range_penalty = y_range_penalty_loss(fake_data, len(real_data))\n",
    "            \n",
    "            # Calculate x range penalty\n",
    "            x_range_penalty = x_range_penalty_loss(fake_data, len(real_data))\n",
    "            \n",
    "            total_g_loss = g_loss + length_penalty * 2 + balance_penalty * 1.50 + sequential_extreme_change_loss_penalty * 1 + y_range_penalty\n",
    "            g_optimizer.zero_grad()\n",
    "            total_g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            total_g_loss += total_g_loss.item()\n",
    "\n",
    "    # Calculate overall accuracy for this epoch\n",
    "    d_accuracy = (correct_real + correct_fake) / (2 * total_samples)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {total_g_loss.item():.4f}, d_accuracy: {d_accuracy:.4f}')\n",
    "    \n",
    "    avg_d_loss = total_d_loss / num_batches\n",
    "    avg_g_loss = total_g_loss / num_batches\n",
    "    train_d_losses.append(avg_d_loss)\n",
    "    train_g_losses.append(avg_g_loss)\n",
    "\n",
    "    #scheduler_g.step()\n",
    "    #scheduler_d.step()\n",
    "    \n",
    "    # Run validation and early stopping check after generator training epochs\n",
    "    if epoch % 2 == 0:\n",
    "        # Validation phase\n",
    "        discriminator.eval()  # Set discriminator to evaluation mode\n",
    "        generator.eval()  # Set generator to evaluation mode\n",
    "        \n",
    "        d_loss_val_total = 0.0\n",
    "        total_g_loss_val = 0.0\n",
    "        length_penalty_val_total = 0.0\n",
    "        balance_penalty_val_total = 0.0\n",
    "        num_val_batches = 0\n",
    "        correct_real_val = 0\n",
    "        correct_fake_val = 0\n",
    "        total_val_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for real_data in val_dataloader:\n",
    "                num_val_batches += 1\n",
    "                real_data = real_data.float().to(device).view(real_data.size(0), -1)  # Flatten and move to GPU\n",
    "                real_labels = torch.ones(len(real_data), 1).to(device)\n",
    "                fake_labels = torch.zeros(len(real_data), 1).to(device)\n",
    "                normalized_real_data = real_data.clone()\n",
    "                normalized_real_data[:, 1::2] = normalize_angles(real_data[:, 1::2])\n",
    "\n",
    "                # Validation for discriminator\n",
    "                real_output = discriminator(normalized_real_data)\n",
    "                z = torch.randn(len(real_data), input_size_generator).to(device)\n",
    "                fake_data = generator(z).view(z.size(0), -1)  # Flatten the fake data\n",
    "                fake_output = discriminator(fake_data)\n",
    "                d_loss_real_val = criterion(real_output, real_labels)\n",
    "                d_loss_fake_val = criterion(fake_output, fake_labels)\n",
    "                d_loss_val = d_loss_real_val + d_loss_fake_val\n",
    "                d_loss_val_total += d_loss_val.item()\n",
    "                \n",
    "                # Calculate validation accuracy\n",
    "                correct_real_val += (real_output > 0.5).sum().item()\n",
    "                correct_fake_val += (fake_output < 0.5).sum().item()\n",
    "                total_val_samples += len(real_data)\n",
    "\n",
    "                # Validation for generator\n",
    "                output = discriminator(fake_data)\n",
    "                g_loss_val = criterion(output, real_labels)\n",
    "                length_penalty_val = length_penalty_loss(fake_data, len(real_data))\n",
    "                # sequential_penalty_val = sequential_penalty_loss(fake_data, len(real_data))\n",
    "                real_lengths_val = torch.norm(real_data.view(len(real_data), -1, 2), dim=2)\n",
    "                fake_lengths_val = torch.norm(fake_data.view(len(fake_data), -1, 2), dim=2)\n",
    "                # difference_loss_val = length_difference_loss(real_lengths_val, fake_lengths_val)\n",
    "                fake_angles_val = fake_data[:, 1::2]\n",
    "                # angle_penalty_val = angle_loss(fake_angles_val)\n",
    "                # angle_range_penalty_val = angle_range_penalty_loss(fake_data)\n",
    "                balance_penalty_val = balance_penalty_loss(fake_angles_val)\n",
    "                sequential_extreme_change_loss_penalty = sequential_extreme_change_loss(fake_data, len(real_data))\n",
    "                \n",
    "                # Calculate y range penalty\n",
    "                y_range_penalty = y_range_penalty_loss(fake_data, len(real_data))\n",
    "                \n",
    "                # Calculate x range penalty\n",
    "                x_range_penalty = x_range_penalty_loss(fake_data, len(real_data))\n",
    "                \n",
    "                total_g_loss_val += g_loss_val.item() + length_penalty_val.item() * 2 + balance_penalty_val.item() * 1.50 + sequential_extreme_change_loss_penalty.item() + y_range_penalty.item()\n",
    "\n",
    "        avg_d_loss_val = d_loss_val_total / num_val_batches\n",
    "        avg_g_loss_val = total_g_loss_val / num_val_batches\n",
    "        val_d_losses.append(avg_d_loss_val)\n",
    "        val_g_losses.append(avg_g_loss_val)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        d_accuracy_val = (correct_real_val + correct_fake_val) / (2 * total_val_samples)\n",
    "\n",
    "        print(f'Validation - Epoch [{epoch+1}/{num_epochs}], d_loss: {avg_d_loss_val:.4f}, g_loss: {avg_g_loss_val:.4f}, d_accuracy_val: {d_accuracy_val:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_g_loss_val < best_loss:\n",
    "            best_loss = avg_g_loss_val\n",
    "            patience_counter = 0\n",
    "            print(\"Validation loss decreased\")\n",
    "            # Optionally save the model here\n",
    "            torch.save(generator.state_dict(), './testsaves/best_generator_test_1.pth')\n",
    "            torch.save(discriminator.state_dict(), './testsaves/best_discriminator_test_1.pth')\n",
    "        else:\n",
    "            print(\"Validation loss increased\")\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "030f79fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/280], d_loss: 10.8270, g_loss: 94414.6484, d_accuracy: 0.4998\n",
      "Epoch [1/280], d_loss: 9.5301, g_loss: 44220.5820, d_accuracy: 0.4995\n",
      "Epoch [2/280], d_loss: 6.6027, g_loss: 34133.0703, d_accuracy: 0.4885\n",
      "Saving plot for epoch 2 at path ../graphs/segments_plot_2.png\n",
      "Validation - Epoch [2/280], d_loss: 0.4802, g_loss: 18209.1660, d_accuracy_val: 1.0000\n",
      "Validation loss decreased\n",
      "Epoch [3/280], d_loss: 5.6544, g_loss: 28451.1699, d_accuracy: 0.5002\n",
      "Epoch [4/280], d_loss: 5.4164, g_loss: 26326.6895, d_accuracy: 0.5279\n",
      "Saving plot for epoch 4 at path ../graphs/segments_plot_4.png\n",
      "Validation - Epoch [4/280], d_loss: 0.6308, g_loss: 13997.1000, d_accuracy_val: 0.9966\n",
      "Validation loss decreased\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 409\u001b[0m\n\u001b[0;32m    407\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m d_loss_real \u001b[38;5;241m+\u001b[39m d_loss_fake \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m gradient_penalty\n\u001b[0;32m    408\u001b[0m d_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 409\u001b[0m \u001b[43md_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m d_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    412\u001b[0m total_d_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\edine\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\edine\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\edine\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "# input_size_discriminator = 1710  # Input size should match the length of each segment for length and angle only\n",
    "# hidden_size_discriminator1 = 256\n",
    "# hidden_size_discriminator2 = 128\n",
    "# hidden_size_discriminator3 = 32\n",
    "# hidden_size_discriminator4 = 16\n",
    "# output_size_discriminator = 1\n",
    "\n",
    "# input_size_generator = 16\n",
    "# hidden_size_generator1 = 64\n",
    "# hidden_size_generator2 = 256\n",
    "# hidden_size_generator3 = 256\n",
    "# hidden_size_generator4 = 512\n",
    "# hidden_size_generator5 = 512\n",
    "# hidden_size_generator6 = 1024\n",
    "# hidden_size_generator7 = 256\n",
    "# output_size_generator = 1710  # Output size should match the size of each segment for length and angle only\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size_discriminator = 1710  # Input size should match the length of each segment for length and angle only\n",
    "hidden_sizes_discriminator = [512, 512, 512, 512, 512]\n",
    "output_size_discriminator = 1\n",
    "\n",
    "input_size_generator = 16\n",
    "hidden_sizes_generator = [512, 512, 512, 512, 512]\n",
    "output_size_generator = 1710  # Output size should match the size of each segment for length and angle only\n",
    "\n",
    "condition_size = 1\n",
    "\n",
    "    \n",
    "# discriminator = FNNDiscriminator(input_size_discriminator, hidden_size_discriminator1,\n",
    "#                                   hidden_size_discriminator2, hidden_size_discriminator3,\n",
    "#                                   hidden_size_discriminator4,\n",
    "#                                   output_size_discriminator).to(device)\n",
    "# generator = FNNGenerator(input_size_generator, hidden_size_generator1, hidden_size_generator2, hidden_size_generator3, hidden_size_generator4, hidden_size_generator5, hidden_size_generator6, hidden_size_generator7, output_size_generator).to(device)\n",
    "\n",
    "#### NON CONDITIONAL\n",
    "# discriminator = FNNDiscriminator(input_size_discriminator, hidden_sizes_discriminator, output_size_discriminator).to(device)\n",
    "# generator = FNNGenerator(input_size_generator, hidden_sizes_generator, output_size_generator).to(device)\n",
    "\n",
    "#### CONDITIONAL\n",
    "discriminator = FNNDiscriminator(input_size_discriminator, condition_size, hidden_sizes_discriminator, output_size_discriminator).to(device)\n",
    "generator = FNNGenerator(input_size_generator, condition_size, hidden_sizes_generator, output_size_generator).to(device)\n",
    "\n",
    "criterion = nn.BCELoss().to(device)  # Binary Cross-Entropy Loss\n",
    "#d_optimizer = optim.Adam(discriminator.parameters(), lr=0.000001, betas=(0.95, 0.999))\n",
    "#g_optimizer = optim.Adam(generator.parameters(), lr=0.00001, betas=(0.65, 0.999))\n",
    "#d_optimizer = optim.Adam(discriminator.parameters(), lr=0.00001)\n",
    "# g_optimizer = optim.Adagrad(generator.parameters(), lr=0.000025)\n",
    "\n",
    "d_optimizer = optim.AdamW(discriminator.parameters(), lr=0.0000482, betas=(0.95, 0.999), weight_decay=1e-4)\n",
    "g_optimizer = optim.AdamW(generator.parameters(), lr=0.000018335, betas=(0.95, 0.999), weight_decay=1e-3)\n",
    "\n",
    "# d_optimizer = optim.AdamW(discriminator.parameters(), lr=0.00005698788315, betas=(0.95, 0.999), weight_decay=1e-2)\n",
    "# g_optimizer = optim.AdamW(generator.parameters(), lr=0.0000164885, betas=(0.95, 0.999), weight_decay=1e-2)\n",
    "\n",
    "# scheduler_g = optim.lr_scheduler.StepLR(g_optimizer, step_size=8, gamma=0.01)\n",
    "# scheduler_d = optim.lr_scheduler.StepLR(d_optimizer, step_size=8, gamma=0.01)\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_data, fake_data, conditions):\n",
    "    alpha = torch.rand(real_data.size(0), 1, device=real_data.device)\n",
    "    alpha = alpha.expand_as(real_data)\n",
    "\n",
    "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "    interpolates.requires_grad_(True)\n",
    "\n",
    "    d_interpolates = discriminator(interpolates, conditions)\n",
    "\n",
    "    fake = torch.ones(d_interpolates.size(), device=real_data.device)\n",
    "    \n",
    "    # skip element 0 of the tensor\n",
    "    # print(\"interpolates requires grad:\", interpolates.requires_grad)\n",
    "    # print(\"d_interpolates shape:\", d_interpolates.shape)\n",
    "    # print(\"fake shape:\", fake.shape)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    \n",
    "    \n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "def normalize_condition(condition, min_val, max_val):\n",
    "    return (condition - min_val) / (max_val - min_val)\n",
    "        \n",
    "def wasserstein_loss(D_real, D_fake):\n",
    "    return D_fake.mean() - D_real.mean()\n",
    "\n",
    "def length_penalty_loss(flat_fake_data, batch_size):\n",
    "    lengths = flat_fake_data.view(batch_size, -1, 2)[:, :, 0]  # Extract length values\n",
    "    # Calculate absolute difference for each length from 0.5 and apply ReLU\n",
    "    penalty = torch.relu(torch.abs(lengths - 0.5))\n",
    "    # Sum the penalties\n",
    "    penalty = torch.sum(penalty)\n",
    "    return penalty\n",
    "\n",
    "def normalize_angles(angles, min_angle=-60, max_angle=80):\n",
    "    # Normalize angles to the range [-1, 1]\n",
    "    return 2 * ((angles - min_angle) / (max_angle - min_angle)) - 1\n",
    "\n",
    "def denormalize_angles(normalized_angles, min_angle=-60, max_angle=80):\n",
    "    # Denormalize angles to the range [min_angle, max_angle]\n",
    "    return ((normalized_angles + 1) / 2) * (max_angle - min_angle) + min_angle\n",
    "\n",
    "def plot_segments_from_json(data, epoch, data2=None):\n",
    "    # Create subplots: 2 rows, 1 column if data2 is available, otherwise just 1 subplot\n",
    "    if data2 is not None:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 3))  # Two subplots: one for data, one for data2\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots(figsize=(7, 3))  # Single subplot for data\n",
    "    \n",
    "    # Plot the first dataset (data) with the original logic\n",
    "    for sample in data:\n",
    "        segments = sample[\"segments\"]\n",
    "        x, y = 0, int(np.random.randint(10, 25))\n",
    "        \n",
    "        for segment in segments:\n",
    "            length = segment[\"length\"]\n",
    "            angle = np.radians(segment[\"angleToNextVector\"])\n",
    "\n",
    "            new_x = x + length * np.cos(angle)\n",
    "            new_y = y + length * np.sin(angle)\n",
    "            segment[\"x\"] = new_x\n",
    "            segment[\"y\"] = new_y\n",
    "            \n",
    "            # Plot a line segment from (x, y) to (new_x, new_y)\n",
    "            ax1.plot([x, new_x], [y, new_y], 'bo-', markersize=5)\n",
    "\n",
    "            # Update the current point\n",
    "            x, y = new_x, new_y\n",
    "            \n",
    "            # Mark the final point with coordinates\n",
    "            if segment == segments[-1]:\n",
    "                ax1.plot(x, y, 'ro')\n",
    "                ax1.text(x, y, f'({int(x)}, {int(y)})')\n",
    "    \n",
    "    ax1.set_aspect('equal', 'box')\n",
    "    ax1.autoscale()\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.set_title('Segments Plot - Data 1')\n",
    "    ax1.axvline(x=430, color='r', linestyle='--')\n",
    "    ax1.set_ylim([-10, 55])\n",
    "    \n",
    "    # If data2 is provided, plot it with the different logic\n",
    "    if data2 is not None:\n",
    "        for sample in data2:\n",
    "            segments = sample[\"segments\"]\n",
    "            \n",
    "            for segment in segments:\n",
    "                x, y = segment[\"x\"], segment[\"y\"]\n",
    "                \n",
    "                # Plot the points and lines\n",
    "                ax2.plot(x, y, 'bo-', markersize=5)  # 'bo-' for blue line with circle markers\n",
    "\n",
    "        ax2.set_aspect('equal', 'box')\n",
    "        ax2.autoscale()\n",
    "        ax2.set_xlabel('X')\n",
    "        ax2.set_ylabel('Y')\n",
    "        ax2.set_title('Segments Plot - Data 2')\n",
    "        ax2.axvline(x=430, color='r', linestyle='--')\n",
    "        ax2.set_ylim([-10, 55])\n",
    "\n",
    "    # Minimize whitespace padding\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    path = f'../graphs/segments_plot_{epoch}.png'\n",
    "    print(f\"Saving plot for epoch {epoch} at path {path}\")\n",
    "    plt.savefig(path, dpi=1200, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close(fig)  # Close the figure to avoid display in some environments\n",
    "\n",
    "# def sequential_penalty_loss(flat_fake_data, batch_size, min_allowed_diff=10):\n",
    "#     angles = flat_fake_data.view(batch_size, -1, 2)[:, :, 1]\n",
    "#     denormalized_angles = denormalize_angles(angles)\n",
    "#     angle_diffs = denormalized_angles[:, 1:] - denormalized_angles[:, :-1]\n",
    "#     penalty = torch.relu(min_allowed_diff - torch.abs(angle_diffs))\n",
    "#     return torch.sum(penalty)\n",
    "\n",
    "def sequential_penalty_loss(flat_fake_data, batch_size, min_allowed_diff=10, min_angle=-45, max_angle=60):\n",
    "    angles = flat_fake_data.view(batch_size, -1, 2)[:, :, 1]\n",
    "    denormalized_angles = denormalize_angles(angles, min_angle, max_angle)\n",
    "    \n",
    "    # Calculate angle differences and account for circular nature\n",
    "    angle_diffs = denormalized_angles[:, 1:] - denormalized_angles[:, :-1]\n",
    "    angle_diffs = (angle_diffs + 180) % 360 - 180  # Convert to range [-180, 180]\n",
    "    \n",
    "    # Calculate penalties\n",
    "    penalty = torch.relu(torch.abs(angle_diffs) - min_allowed_diff)\n",
    "    return torch.sum(penalty)\n",
    "\n",
    "def angle_loss(normalized_angles, min_allowed_diff_normalized=0.001, sequence_length=4):\n",
    "    \"\"\"\n",
    "    Compute the angle loss to penalize long sequences of small differences in normalized angles.\n",
    "    \n",
    "    Parameters:\n",
    "    normalized_angles (torch.Tensor): The tensor of normalized angles.\n",
    "    min_allowed_diff_normalized (float): The minimum allowed difference in normalized values.\n",
    "    sequence_length (int): The length of sequences to penalize if differences are below the min allowed difference.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The computed angle loss.\n",
    "    \"\"\"\n",
    "    # Compute the absolute differences between consecutive normalized angles\n",
    "    angle_diffs = torch.abs(normalized_angles[:, 1:] - normalized_angles[:, :-1])\n",
    "    \n",
    "    # Create mask for where the angle differences are below the minimum allowed deviation\n",
    "    small_diff_mask = angle_diffs < min_allowed_diff_normalized\n",
    "    \n",
    "    # Create a penalty mask for sequences of small differences\n",
    "    seq_mask = torch.nn.functional.conv1d(small_diff_mask.float().unsqueeze(1), \n",
    "                                          torch.ones((1, 1, sequence_length)).to(normalized_angles.device), \n",
    "                                          padding=sequence_length-1)\n",
    "    penalty_mask = (seq_mask >= sequence_length).float()\n",
    "    \n",
    "    # Sum up the penalties for all differences\n",
    "    total_penalty = torch.sum(penalty_mask)\n",
    "    \n",
    "    return total_penalty\n",
    "\n",
    "def balance_penalty_loss(normalized_angles):\n",
    "    # Count the number of positive and negative angles\n",
    "    positive_count = torch.sum(normalized_angles > -0.1428, dim=1)\n",
    "    negative_count = torch.sum(normalized_angles < -0.1428, dim=1)\n",
    "    \n",
    "    # Penalize the difference between positive and negative counts\n",
    "    imbalance_penalty = torch.abs(positive_count - negative_count)\n",
    "    \n",
    "    return torch.sum(imbalance_penalty)\n",
    "\n",
    "def angle_range_penalty_loss(normalized_fake_data):\n",
    "    angles = normalized_fake_data[:, 1::2]  # Extract angle values\n",
    "    # Penalize angles outside the range [-1, 1]\n",
    "    penalty_outside_range = torch.relu(-angles) + torch.relu(angles - 1)\n",
    "    # Sum the penalties\n",
    "    total_penalty = torch.sum(penalty_outside_range)\n",
    "    return total_penalty\n",
    "\n",
    "def y_range_penalty_loss(fake_data, batch_size):\n",
    "    x,y = calculate_cumulative_coordinates_parallel(fake_data , batch_size)\n",
    "    \n",
    "    # Penalize y values outside the range (-18,18)\n",
    "    penalty_outside_range = torch.relu(-y) + torch.relu(y - 18) \n",
    "    \n",
    "    # Sum the penalties\n",
    "    total_penalty = torch.sum(penalty_outside_range)\n",
    "    \n",
    "    return total_penalty\n",
    "\n",
    "def x_range_penalty_loss(fake_data, batch_size):\n",
    "    x, y = calculate_cumulative_coordinates_parallel(fake_data, batch_size)\n",
    "    \n",
    "    # Take the last x-coordinate and penalize if it's less than 425\n",
    "    penalty_outside_range = torch.relu(425 - x[:, -1])\n",
    "    \n",
    "    # Compute the maximum penalty across the batch\n",
    "    total_penalty = torch.max(penalty_outside_range)\n",
    "    \n",
    "    return total_penalty\n",
    "    \n",
    "\n",
    "def calculate_cumulative_coordinates_parallel(flat_fake_data, batch_size):\n",
    "    angles = flat_fake_data.view(batch_size, -1, 2)[:, :, 1]\n",
    "    lengths = flat_fake_data.view(batch_size, -1, 2)[:, :, 0]\n",
    "    \n",
    "    # denormalize angles\n",
    "    angles = denormalize_angles(angles)\n",
    "    \n",
    "    # Convert angles from degrees to radians\n",
    "    angles = angles * (torch.pi / 180.0)\n",
    "    \n",
    "    # Calculate the delta x and y for each segment\n",
    "    delta_x = lengths * torch.cos(angles)\n",
    "    delta_y = lengths * torch.sin(angles)\n",
    "    \n",
    "    # Use cumulative sum to get the coordinates\n",
    "    x = torch.cumsum(delta_x, dim=1)\n",
    "    y = torch.cumsum(delta_y, dim=1)\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "\n",
    "\n",
    "def length_difference_loss(real_lengths, fake_lengths):\n",
    "    real_avg_length = torch.mean(real_lengths)\n",
    "    fake_avg_length = torch.mean(fake_lengths)\n",
    "    return torch.abs(real_avg_length - fake_avg_length)\n",
    "\n",
    "def sequential_extreme_change_loss(flat_fake_data, batch_size, min_angle=-45, max_angle=60, max_diff=30):\n",
    "    angles = flat_fake_data.view(batch_size, -1, 2)[:, :, 1]\n",
    "    \n",
    "    # Denormalize angles to their actual range\n",
    "    denormalized_angles = denormalize_angles(angles, min_angle, max_angle)\n",
    "    \n",
    "    # Compute the differences between consecutive angles\n",
    "    angle_diffs = torch.abs(denormalized_angles[:, 1:] - denormalized_angles[:, :-1])\n",
    "    \n",
    "    # Calculate the penalty for each angle difference\n",
    "    penalties = torch.relu(angle_diffs - max_diff)\n",
    "    \n",
    "    # Sum up the penalties for all differences\n",
    "    total_penalty = torch.sum(penalties)\n",
    "    \n",
    "    return total_penalty\n",
    "\n",
    "\n",
    "# def sequential_extreme_change_loss(flat_fake_data, batch_size):\n",
    "#     angles = flat_fake_data.view(batch_size, -1, 2)[:, :, 1]\n",
    "    \n",
    "#     # Compute the absolute differences between consecutive angles\n",
    "#     angle_diffs = torch.abs(angles[:, 1:] - angles[:, :-1])\n",
    "    \n",
    "#     # Create mask for where the angle differences are above the maximum allowed deviation which is 20 degrees normalized between -1 and 1 on a scale of -45 to 60\n",
    "#     normalized_max_diff = 0.3333  # 20 degrees normalized between -1 and 1\n",
    "#     extreme_change_mask = angle_diffs > normalized_max_diff\n",
    "    \n",
    "#     # Create a penalty mask for sequences of extreme changes\n",
    "#     seq_mask = torch.nn.functional.conv1d(extreme_change_mask.float().unsqueeze(1),\n",
    "#                                              torch.ones((1, 1, 3)).to(angles.device),\n",
    "#                                              padding=2)\n",
    "    \n",
    "#     penalty_mask = (seq_mask >= 2).float() # 2 consecutive extreme changes\n",
    "    \n",
    "#     # Sum up the penalties for all differences\n",
    "#     total_penalty = torch.sum(penalty_mask)\n",
    "    \n",
    "#     return total_penalty\n",
    "\n",
    "# Initialize lists to store losses for plotting\n",
    "train_d_losses = []\n",
    "train_g_losses = []\n",
    "val_d_losses = []\n",
    "val_g_losses = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 280\n",
    "lambda_penalty = 3.0  # Weight for the length penalty loss\n",
    "lambda_difference = 10.0  # Weight for the length difference loss\n",
    "lambda_angle = 6.0  # Weight for the angle penalty loss\n",
    "lambda_angle_range = 15.0  # Weight for the angle range penalty loss\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience = 6\n",
    "patience_counter = 0\n",
    "    #--------------------------------------------------------- CONDITIONAL ---------------------------------------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    discriminator.train()  # Set discriminator to training mode\n",
    "    generator.train()  # Set generator to training mode\n",
    "    \n",
    "    total_d_loss = 0.0\n",
    "    total_g_loss = 0.0\n",
    "    num_batches = 0\n",
    "    correct_real = 0\n",
    "    correct_fake = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for i, (real_data, conditions) in enumerate(train_dataloader):\n",
    "        num_batches += 1\n",
    "        \n",
    "        assert len(real_data) == len(conditions) # Ensure the batch size is the same for both real data and conditions\n",
    "        \n",
    "        # if num_batches == len(train_dataloader) / 4:\n",
    "        #     print(f'Batch {i+1}/{len(train_dataloader)} which is 25% of the dataset')\n",
    "        # if num_batches == len(train_dataloader) / 2:\n",
    "        #     print(f'Batch {i+1}/{len(train_dataloader)} which is 50% of the dataset')\n",
    "        # if num_batches == len(train_dataloader) * 3 / 4:\n",
    "        #     print(f'Batch {i+1}/{len(train_dataloader)} which is 75% of the dataset')\n",
    "        \n",
    "        real_data = real_data.float().to(device).view(real_data.size(0), -1)  # Flatten and move to GPU\n",
    "        conditions = conditions.float().to(device).view(real_data.size(0), 1)  # Ensure conditions are properly shaped and on GPU\n",
    "        real_labels = torch.ones(len(real_data), 1).to(device)\n",
    "        fake_labels = torch.zeros(len(real_data), 1).to(device)\n",
    "        normalized_real_data = real_data.clone()\n",
    "        normalized_real_data[:, 1::2] = normalize_angles(real_data[:, 1::2])\n",
    "        \n",
    "        # Noise injection to real data (out-of-place)\n",
    "        noisy_real_data = real_data + torch.normal(0, 0.05, real_data.shape).to(device)\n",
    "        \n",
    "        # Noise injection to conditions (out-of-place)\n",
    "        noisy_conditions = conditions + torch.normal(0, 0.05, conditions.shape).to(device)\n",
    "        \n",
    "        # Train discriminator\n",
    "        real_output = discriminator(normalized_real_data, noisy_conditions)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "        z = torch.randn(len(real_data), input_size_generator).to(device)  # Generate random noise\n",
    "        fake_data = generator(z, noisy_conditions).view(z.size(0), -1)  # Flatten the fake data\n",
    "\n",
    "        # Noise injection to fake data (out-of-place)\n",
    "        noisy_fake_data = fake_data + torch.normal(0, 0.65, fake_data.shape).to(device)\n",
    "        \n",
    "        fake_output = discriminator(noisy_fake_data, noisy_conditions)\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "        \n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, noisy_real_data, noisy_fake_data, noisy_conditions)\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake + 10 * gradient_penalty\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        total_d_loss += d_loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct_real += (real_output > 0.5).sum().item()\n",
    "        correct_fake += (fake_output < 0.5).sum().item()\n",
    "        total_samples += len(real_data)\n",
    "\n",
    "        # Train generator every 2 steps\n",
    "        if i % 2 == 0:\n",
    "            z = torch.randn(len(real_data), input_size_generator).to(device)\n",
    "            fake_data = generator(z, conditions).view(z.size(0), -1)\n",
    "\n",
    "            # Noise injection to fake data (out-of-place)\n",
    "            noisy_fake_data = fake_data + torch.normal(0, 0.65, fake_data.shape).to(device)\n",
    "\n",
    "            output = discriminator(noisy_fake_data, conditions)\n",
    "            g_loss = criterion(output, real_labels)\n",
    "            length_penalty = length_penalty_loss(fake_data, len(real_data))\n",
    "            \n",
    "            real_lengths = torch.norm(real_data.view(len(real_data), -1, 2), dim=2)\n",
    "            fake_lengths = torch.norm(fake_data.view(len(fake_data), -1, 2), dim=2)\n",
    "            fake_angles = fake_data[:, 1::2]\n",
    "            balance_penalty = balance_penalty_loss(fake_angles)\n",
    "            sequential_extreme_change_loss_penalty = sequential_extreme_change_loss(fake_data, len(real_data))\n",
    "            \n",
    "            # Calculate sequential penalty\n",
    "            sequential_penalty = sequential_penalty_loss(fake_data, len(real_data))\n",
    "            \n",
    "            # Calculate y range penalty\n",
    "            y_range_penalty = y_range_penalty_loss(fake_data, len(real_data))\n",
    "            \n",
    "            # Calculate x range penalty\n",
    "            x_range_penalty = x_range_penalty_loss(fake_data, len(real_data))\n",
    "            \n",
    "            total_g_loss = g_loss + length_penalty * 3  + sequential_extreme_change_loss_penalty * 1.15 + y_range_penalty + x_range_penalty * 1.5 + balance_penalty * 1.5\n",
    "            g_optimizer.zero_grad()\n",
    "            total_g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            total_g_loss += total_g_loss.item()\n",
    "\n",
    "    # Calculate overall accuracy for this epoch\n",
    "    d_accuracy = (correct_real + correct_fake) / (2 * total_samples)\n",
    "\n",
    "    print(f'Epoch [{epoch}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {total_g_loss.item():.4f}, d_accuracy: {d_accuracy:.4f}')\n",
    "    \n",
    "    avg_d_loss = total_d_loss / num_batches\n",
    "    avg_g_loss = total_g_loss / num_batches\n",
    "    train_d_losses.append(avg_d_loss)\n",
    "    train_g_losses.append(avg_g_loss)\n",
    "\n",
    "    # Run validation and early stopping check after generator training epochs\n",
    "    if epoch % 2 == 0 and epoch > 0:\n",
    "        # Validation phase\n",
    "        discriminator.eval()  # Set discriminator to evaluation mode\n",
    "        generator.eval()  # Set generator to evaluation mode\n",
    "        \n",
    "        d_loss_val_total = 0.0\n",
    "        total_g_loss_val = 0.0\n",
    "        num_val_batches = 0\n",
    "        correct_real_val = 0\n",
    "        correct_fake_val = 0\n",
    "        total_val_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for real_data, conditions in val_dataloader:\n",
    "                num_val_batches += 1\n",
    "                real_data = real_data.float().to(device).view(real_data.size(0), -1)  # Flatten and move to GPU\n",
    "                conditions = conditions.float().to(device).view(conditions.size(0), -1)  # Ensure conditions are properly shaped and on GPU\n",
    "                real_labels = torch.ones(len(real_data), 1).to(device) * 0.9\n",
    "                fake_labels = torch.zeros(len(real_data), 1).to(device) * 0.1\n",
    "                normalized_real_data = real_data.clone()\n",
    "                normalized_real_data[:, 1::2] = normalize_angles(real_data[:, 1::2])\n",
    "\n",
    "                # Validation for discriminator\n",
    "                real_output = discriminator(normalized_real_data, conditions)\n",
    "                z = torch.randn(len(real_data), input_size_generator).to(device)\n",
    "                fake_data = generator(z, conditions).view(z.size(0), -1)  # Flatten the fake data\n",
    "                fake_output = discriminator(fake_data, conditions)\n",
    "                d_loss_real_val = criterion(real_output, real_labels)\n",
    "                d_loss_fake_val = criterion(fake_output, fake_labels)\n",
    "                # gradient_penalty_val = compute_gradient_penalty(discriminator, real_data, fake_data, conditions)\n",
    "                d_loss_val = d_loss_real_val + d_loss_fake_val\n",
    "                d_loss_val_total += d_loss_val.item()\n",
    "                \n",
    "                # Calculate validation accuracy\n",
    "                correct_real_val += (real_output > 0.5).sum().item()\n",
    "                correct_fake_val += (fake_output < 0.5).sum().item()\n",
    "                total_val_samples += len(real_data)\n",
    "\n",
    "                # Validation for generator\n",
    "                output = discriminator(fake_data, conditions)\n",
    "                g_loss_val = criterion(output, real_labels)\n",
    "                length_penalty_val = length_penalty_loss(fake_data, len(real_data))\n",
    "                real_lengths_val = torch.norm(real_data.view(len(real_data), -1, 2), dim=2)\n",
    "                fake_lengths_val = torch.norm(fake_data.view(len(fake_data), -1, 2), dim=2)\n",
    "                fake_angles_val = fake_data[:, 1::2]\n",
    "                balance_penalty_val = balance_penalty_loss(fake_angles_val)\n",
    "                sequential_extreme_change_loss_penalty = sequential_extreme_change_loss(fake_data, len(real_data))\n",
    "                \n",
    "                # Calculate y range penalty\n",
    "                y_range_penalty = y_range_penalty_loss(fake_data, len(real_data))\n",
    "                \n",
    "                # Calculate x range penalty\n",
    "                x_range_penalty = x_range_penalty_loss(fake_data, len(real_data))\n",
    "                \n",
    "                total_g_loss_val += g_loss_val.item() + length_penalty_val.item() * 3 + sequential_extreme_change_loss_penalty.item() * 1.15 + y_range_penalty.item() + x_range_penalty.item() * 1.5 + balance_penalty_val.item() * 1.5\n",
    "        \n",
    "            min_val = torch.tensor([0.0]).to(device)\n",
    "            max_val = torch.tensor([1.0]).to(device)\n",
    "\n",
    "            z = torch.randn(1, input_size_generator).to(device)  # Generate random noise\n",
    "            specific_condition_value = 256\n",
    "            dummy_condition = torch.tensor([[specific_condition_value]]).to(device)  # Fill tensor with specific value\n",
    "            # Normalize the condition value\n",
    "            normalized_condition = normalize_condition(specific_condition_value, min_val, max_val)\n",
    "            dummy_condition = normalized_condition  # Use the normalized condition value\n",
    "            fake_data = generator(z, dummy_condition)  # Pass both noise and condition\n",
    "            fake_data = fake_data.view(fake_data.size(0), 855, 2)\n",
    "\n",
    "        fake_data[:, :, 1] = denormalize_angles(fake_data[:, :, 1])\n",
    "        generated_data = []\n",
    "\n",
    "        for sample in fake_data:\n",
    "            segment_data = []\n",
    "            for vector in sample:\n",
    "                segment_data.append({\n",
    "                    \"length\": vector[0].item(),\n",
    "                    \"angleToNextVector\": vector[1].item()\n",
    "                })\n",
    "            generated_data.append({\"segments\": segment_data})\n",
    "            \n",
    "        #print(generated_data)\n",
    "\n",
    "        with open('../validation-data/rand-ground-4.json', 'r') as f:\n",
    "            temp_ground_data = json.load(f)\n",
    "            temp_segments = temp_ground_data['segments']\n",
    "            plot_segments_from_json(generated_data, epoch, [{\"segments\": temp_segments}])\n",
    "        # if epoch > 12:\n",
    "        #     with open('../validation-data/perlin-ground.json', 'r') as f:\n",
    "        #         temp_ground_data = json.load(f)\n",
    "        #         temp_segments = temp_ground_data['segments']\n",
    "        #         plot_segments_from_json(generated_data, epoch, [{\"segments\": temp_segments}])\n",
    "        # else:\n",
    "        #     plot_segments_from_json(generated_data, epoch)\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        avg_d_loss_val = d_loss_val_total / num_val_batches\n",
    "        avg_g_loss_val = total_g_loss_val / num_val_batches\n",
    "        val_d_losses.append(avg_d_loss_val)\n",
    "        val_g_losses.append(avg_g_loss_val)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        d_accuracy_val = (correct_real_val + correct_fake_val) / (2 * total_val_samples)\n",
    "\n",
    "        print(f'Validation - Epoch [{epoch}/{num_epochs}], d_loss: {avg_d_loss_val:.4f}, g_loss: {avg_g_loss_val:.4f}, d_accuracy_val: {d_accuracy_val:.4f}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_g_loss_val < best_loss:\n",
    "            best_loss = avg_g_loss_val\n",
    "            patience_counter = 0\n",
    "            print(\"Validation loss decreased\")\n",
    "            # Optionally save the model here\n",
    "            torch.save(generator.state_dict(), './testsaves/best_generator_test_1.pth')\n",
    "            torch.save(discriminator.state_dict(), './testsaves/best_discriminator_test_1.pth')\n",
    "        else:\n",
    "            print(\"Validation loss increased\")\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "# Play a sound when done\n",
    "winsound.Beep(320, 250)  # Beep at 440 Hz for 1000 ms\n",
    "winsound.Beep(320, 250)  # Beep at 440 Hz for 1000 ms   \n",
    "    # Convert losses to CPU and numpy arrays for plotting\n",
    "    # train_d_losses_cpu = [loss.detach().cpu().numpy() if torch.is_tensor(loss) else loss for loss in train_d_losses]\n",
    "    # train_g_losses_cpu = [loss.detach().cpu().numpy() if torch.is_tensor(loss) else loss for loss in train_g_losses]\n",
    "    # val_d_losses_cpu = [loss.detach().cpu().numpy() if torch.is_tensor(loss) else loss for loss in val_d_losses]\n",
    "    # val_g_losses_cpu = [loss.detach().cpu().numpy() if torch.is_tensor(loss) else loss for loss in val_g_losses]\n",
    "\n",
    "    # Plot the losses\n",
    "    # plt.plot(train_d_losses_cpu, label='Train D Loss')\n",
    "    # plt.plot(val_d_losses_cpu, label='Validation D Loss')\n",
    "    # plt.plot(train_g_losses_cpu, label='Train G Loss')\n",
    "    # plt.plot(val_g_losses_cpu, label='Validation G Loss')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.title('Training and Validation Losses')\n",
    "    # plt.show()\n",
    "# Generate new data after training\n",
    "# z = torch.randn(1, input_size_generator).unsqueeze(1).repeat(1, 855, 1).to(device)  # Generate random noise\n",
    "# z = torch.randn(len(real_data), input_size_generator).to(device) \n",
    "# fake_data = generator(z)\n",
    "# fake_data = fake_data.view(fake_data.size(0), 855, 2)\n",
    "\n",
    "# generated_data = []\n",
    "\n",
    "# for sample in fake_data:\n",
    "#     segment_data = []\n",
    "#     for vector in sample:\n",
    "#         segment_data.append({\n",
    "#             \"length\": vector[0].item(),\n",
    "#             \"angleToNextVector\": vector[1].item()\n",
    "#         })\n",
    "#     generated_data.append({\"segments\": segment_data})\n",
    "\n",
    "# print(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segments_from_json(data):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for sample in data:\n",
    "        print(\"sample\", sample)\n",
    "        segments = sample[\"segments\"]\n",
    "        # Starting point x is 0 while y is random number between 10 and 20\n",
    "        x, y = 0, int(np.random.randint(10, 25))\n",
    "        \n",
    "        for segment in segments:\n",
    "            length = segment[\"length\"]\n",
    "            tmpangle = segment[\"angleToNextVector\"]\n",
    "            #while tmpangle < 0:\n",
    "            #    tmpangle = tmpangle + 360\n",
    "            angle = np.radians(segment[\"angleToNextVector\"])  # Convert angle to radians\n",
    "\n",
    "            new_x = x + length * np.cos(angle)\n",
    "            new_y = y + length * np.sin(angle) \n",
    "            segment[\"x\"] = new_x\n",
    "            segment[\"y\"] = new_y\n",
    "            \n",
    "            # Plot a line segment from (x, y) to (new_x, new_y)\n",
    "            ax.plot([x, new_x], [y, new_y], 'bo-', markersize=5)  # 'bo-' for blue line with circle markers\n",
    "\n",
    "            # Update the current point\n",
    "            x, y = new_x, new_y\n",
    "            # if its the final segment, plot the final point and mark the x and y\n",
    "            if segment == segments[-1]:\n",
    "                ax.plot(x, y, 'ro')\n",
    "                ax.text(x, y, f'({int(x)}, {int(y)})')\n",
    "                \n",
    "            \n",
    "            \n",
    "\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.autoscale()\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title('Segments Plot')\n",
    "    # draw horziontal line on the last y\n",
    "    ax.axvline(x=430, color='r', linestyle='--')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f4504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, generate new fake data\n",
    "# z = torch.randn(1, input_size_generator).unsqueeze(1).repeat(1, 855, 1).to(device)  # Generate random noise\n",
    "\n",
    "# Evaulate the model after training and plot the generated data\n",
    "\n",
    "# Set the models to evaluation mode\n",
    "\n",
    "# Generate new data after training\n",
    "# Set the models to evaluation mode\n",
    "discriminator.eval()\n",
    "generator.eval()\n",
    "\n",
    "# Generate new data after training\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    z = torch.randn(1, input_size_generator).to(device)  # Generate random noise\n",
    "    specific_condition_value = 0.25\n",
    "    dummy_condition = torch.full((1, 1), specific_condition_value).to(device)  # Fill tensor with specific value\n",
    "    \n",
    "    fake_data = generator(z, dummy_condition)  # Pass both noise and condition\n",
    "    fake_data = fake_data.view(fake_data.size(0), 855, 2)\n",
    "\n",
    "fake_data[:, :, 1] = denormalize_angles(fake_data[:, :, 1])\n",
    "\n",
    "generated_data = []\n",
    "\n",
    "for sample in fake_data:\n",
    "    segment_data = []\n",
    "    for vector in sample:\n",
    "        segment_data.append({\n",
    "            \"length\": vector[0].item(),\n",
    "            \"angleToNextVector\": vector[1].item()\n",
    "        })\n",
    "    generated_data.append({\"segments\": segment_data})\n",
    "\n",
    "print(generated_data)\n",
    "\n",
    "def plot_segments_from_json(data):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for sample in data:\n",
    "        print(\"sample\", sample)\n",
    "        segments = sample[\"segments\"]\n",
    "        x, y = 0, 0  # Starting point\n",
    "        \n",
    "        for segment in segments:\n",
    "            length = segment[\"length\"]\n",
    "            tmpangle = segment[\"angleToNextVector\"]\n",
    "            #while tmpangle < 0:\n",
    "            #    tmpangle = tmpangle + 360\n",
    "            angle = np.radians(segment[\"angleToNextVector\"])  # Convert angle to radians\n",
    "\n",
    "            new_x = x + length * np.cos(angle)\n",
    "            new_y = y + length * np.sin(angle) \n",
    "            segment[\"x\"] = new_x\n",
    "            segment[\"y\"] = new_y\n",
    "            \n",
    "            # Plot a line segment from (x, y) to (new_x, new_y)\n",
    "            ax.plot([x, new_x], [y, new_y], 'bo-', markersize=5)  # 'bo-' for blue line with circle markers\n",
    "\n",
    "            # Update the current point\n",
    "            x, y = new_x, new_y\n",
    "\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.autoscale()\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title('Segments Plot')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_segments_from_json(generated_data)\n",
    "\n",
    "# Save the models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#read ground-1.json and plot the segment\n",
    "\n",
    "# with open('../learning-data/ground-2.json', 'r') as f:\n",
    "#     temp_ground_data = json.load(f)\n",
    "#     temp_segments = temp_ground_data['segments']\n",
    "#     plot_segments_from_json([{\"segments\": temp_segments}])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7acd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save latest trained discriminator and generator models\n",
    "torch.save(discriminator.state_dict(), 'discriminator-best-so-far-test.pth')\n",
    "torch.save(generator.state_dict(), 'generator-best-so-far-test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best trained discriminator and generator models\n",
    "new_discriminator_save = FNNDiscriminator(1710, 1, hidden_sizes_discriminator, output_size_discriminator).to(device)\n",
    "new_generator_save = FNNGenerator(input_size_generator,1, hidden_sizes_generator, output_size_generator).to(device)\n",
    "\n",
    "new_discriminator_save.load_state_dict(torch.load('./testsaves/best_discriminator_test_1.pth'))\n",
    "new_generator_save.load_state_dict(torch.load('./testsaves/best_generator_test_1.pth'))\n",
    "\n",
    "torch.save(new_discriminator_save.state_dict(), 'discriminator-best-test_6.pth')\n",
    "torch.save(new_generator_save.state_dict(), 'generator-best-test_6.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51df12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#new_discriminator = FNNDiscriminator(input_size_discriminator, hidden_size_discriminator1, hidden_size_discriminator2, hidden_size_discriminator3, hidden_size_discriminator4, output_size_discriminator).to(device)\n",
    "#new_generator = FNNGenerator(input_size_generator, hidden_size_generator1, hidden_size_generator2, hidden_size_generator3, hidden_size_generator4, hidden_size_generator5, hidden_size_generator6, hidden_size_generator7, output_size_generator).to(device)\n",
    "\n",
    "\n",
    "new_discriminator = FNNDiscriminator(input_size_discriminator, 1, hidden_sizes_discriminator, output_size_discriminator).to(device)\n",
    "new_generator = FNNGenerator(input_size_generator, 1, hidden_sizes_generator, output_size_generator).to(device)\n",
    "\n",
    "new_discriminator.load_state_dict(torch.load('./testsaves/best_discriminator_test_1.pth'))\n",
    "new_generator.load_state_dict(torch.load('./testsaves/best_generator_test_1.pth'))\n",
    "\n",
    "# Generate new data after training\n",
    "# z = torch.randn(1, input_size_generator).unsqueeze(1).repeat(1, 855, 1).to(device)  # Generate random noise\n",
    "new_discriminator.eval()\n",
    "new_generator.eval()\n",
    "#min_val = torch.tensor([0.2211]).to(device) \n",
    "#max_val = torch.tensor([0.6012]).to(device) \n",
    "min_val = torch.tensor([0.0]).to(device)\n",
    "max_val = torch.tensor([1.0]).to(device)\n",
    "def normalize_condition(condition, min_val, max_val):\n",
    "    return (condition - min_val) / (max_val - min_val)\n",
    "# Generate new data after training\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    z = torch.randn(1, input_size_generator).to(device)  # Generate random noise\n",
    "    specific_condition_value = 256\n",
    "    dummy_condition = torch.tensor([[specific_condition_value]]).to(device)  # Fill tensor with specific value\n",
    "    # Normalize the condition value\n",
    "    normalized_condition = normalize_condition(specific_condition_value, min_val, max_val)\n",
    "    dummy_condition = normalized_condition  # Use the normalized condition value\n",
    "    fake_data = new_generator(z, dummy_condition)  # Pass both noise and condition\n",
    "    fake_data = fake_data.view(fake_data.size(0), 855, 2)\n",
    "\n",
    "fake_data[:, :, 1] = denormalize_angles(fake_data[:, :, 1])\n",
    "generated_data = []\n",
    "\n",
    "for sample in fake_data:\n",
    "    segment_data = []\n",
    "    for vector in sample:\n",
    "        segment_data.append({\n",
    "            \"length\": vector[0].item(),\n",
    "            \"angleToNextVector\": vector[1].item()\n",
    "        })\n",
    "    generated_data.append({\"segments\": segment_data})\n",
    "    \n",
    "print(generated_data)\n",
    "# Example usage:\n",
    "plot_segments_from_json(generated_data)\n",
    "print(generated_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
